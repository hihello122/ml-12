{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8504bcfb-7b3d-4bf9-a494-2e7f94570429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c22921ce-9380-4836-8a1f-6758da326b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainConfig(model_name_or_path='meta-llama/Meta-Llama-3-8B-Instruct', train_jsonl='prepared_data/train.jsonl', output_dir='./model', max_length=200, per_device_train_batch_size=1, gradient_accumulation_steps=16, num_train_epochs=3, learning_rate=0.0002, warmup_ratio=0.03, logging_steps=50, save_steps=500, seed=42, dataloader_num_workers=2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    # 1) 모델 & 데이터\n",
    "    model_name_or_path: str = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # 원하는 베이스 LLM 이름\n",
    "    train_jsonl: str = \"prepared_data/train.jsonl\"          # 학습시킬 json prompt\n",
    "    \n",
    "    # 결과 저장 위치: EBS 볼륨 내 현재 작업 디렉토리 기준 \"./model\"\n",
    "    output_dir: str = \"./model\"\n",
    "\n",
    "    # 2) 토크나이즈 & 길이\n",
    "    max_length: int = 200\n",
    "\n",
    "    # 3) 학습 하이퍼파라미터\n",
    "    per_device_train_batch_size: int = 1\n",
    "    gradient_accumulation_steps: int = 16\n",
    "    num_train_epochs: int = 3\n",
    "    learning_rate: float = 2e-4\n",
    "    warmup_ratio: float = 0.03\n",
    "\n",
    "    # 4) 로깅 & 저장\n",
    "    logging_steps: int = 50\n",
    "    save_steps: int = 500\n",
    "    seed: int = 42\n",
    "    # 5) 기타\n",
    "    dataloader_num_workers: int = 2  # Colab이면 0~2 정도로 유지\n",
    "\n",
    "    # device_map=\"auto\"\n",
    "    # load_in_4bit=True\n",
    "\n",
    "cfg = TrainConfig()\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2f9225a-5329-42df-9750-386224287c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_jsonl_path(path: str) -> str:\n",
    "    \"\"\"\n",
    "    - s3://bucket/key 형식이면 /tmp/train.jsonl 로 다운로드 후 해당 경로 반환\n",
    "    - 그 외에는 로컬 경로 그대로 반환\n",
    "    \"\"\"\n",
    "    if path.startswith(\"s3://\"):\n",
    "        no_scheme = path[5:]\n",
    "        bucket, key = no_scheme.split(\"/\", 1)\n",
    "\n",
    "        local_path = \"/tmp/train.jsonl\"\n",
    "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "\n",
    "        s3 = boto3.client(\"s3\")\n",
    "        s3.download_file(bucket, key, local_path)\n",
    "        print(f\"[INFO] downloaded {path} -> {local_path}\")\n",
    "        return local_path\n",
    "    else:\n",
    "        return path\n",
    "\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "seed_everything(cfg.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc4ead72-8cc8-425f-92cb-5ddb5bd09d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonlSftDataset(Dataset):\n",
    "    \"\"\"\n",
    "    SFT용 JSONL Dataset.\n",
    "    각 row는 {\"prompt\": \"...\", \"completion\": \"...\"} 형태여야 함.\n",
    "    반환되는 input_ids, attention_mask, labels는 모두 동일 길이를 보장해야 한다.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, jsonl_path, tokenizer, max_length=1024):\n",
    "        self.samples = []\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                obj = json.loads(line)\n",
    "                self.samples.append(obj)\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.samples[idx]\n",
    "        prompt = \" \".join(str(item[\"prompt\"]).split())\n",
    "        completion = \" \".join(str(item[\"completion\"]).split())\n",
    "\n",
    "        # 1) 프롬프트 + completion 합치기\n",
    "        full_text = prompt + completion\n",
    "\n",
    "        # 2) tokenizer로 일괄 encoding (truncation=True)\n",
    "        enc = self.tokenizer(\n",
    "            full_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "\n",
    "        input_ids = enc[\"input_ids\"]\n",
    "        attn_mask = enc[\"attention_mask\"]\n",
    "\n",
    "        # 3) prompt만 tokenize해서 길이 파악\n",
    "        prompt_ids = self.tokenizer(\n",
    "            prompt,\n",
    "            add_special_tokens=False,\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        # BOS 토큰 보정\n",
    "        bos_extra = 1 if (len(input_ids) > 0 and input_ids[0] == self.tokenizer.bos_token_id) else 0\n",
    "\n",
    "        prompt_len = len(prompt_ids) + bos_extra\n",
    "\n",
    "        # 4) labels = input_ids 복사\n",
    "        labels = input_ids.copy()\n",
    "\n",
    "        # prompt 구간 -100\n",
    "        for i in range(prompt_len):\n",
    "            if i < len(labels):\n",
    "                labels[i] = -100\n",
    "\n",
    "        # ※ 여기서 중요한 부분: \n",
    "        # input_ids, attention_mask, labels 모두 정확히 동일 길이\n",
    "        # collator가 나중에 batch padding 할 것이므로 Dataset에서는 길이를 맞추기만 하면 된다.\n",
    "\n",
    "        assert len(input_ids) == len(labels), f\"label mismatch: {len(input_ids)} vs {len(labels)}\"\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attn_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28ab7b73-e2b6-4474-9a73-270f3035a7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access OK!\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "try:\n",
    "    hf_hub_download(\n",
    "        repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        filename=\"config.json\"\n",
    "    )\n",
    "    print(\"Access OK!\")\n",
    "except Exception as e:\n",
    "    print(\"Access error:\", e)\n",
    "\n",
    "import requests\n",
    "\n",
    "HF_TOKEN = \"hf_xxxxxxxxx\"   # 너의 것 넣기\n",
    "headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
    "\n",
    "url = \"https://huggingface.co/api/models/meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "res = requests.get(url, headers=headers)\n",
    "\n",
    "print(res.status_code)\n",
    "# print(res.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a545310c-d3b3-4e9d-8452-496e288505be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 토크나이저\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    cfg.model_name_or_path,\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "# pad 토큰 세팅 (없는 모델들 대비)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "094d7b3b-5455-47b4-8b1b-ae8910bee621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd3e5f7d27147669cda996a1bdec583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef1937df1cc4c1bb22e01d018bc67a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f202adced642a88eb737a24a55f6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c952b89963248a895374f6e9439b582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae4c8b4e00b4f32b3e09e50c2c4ecd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06e7545a6264ad1bef58f8ea7dfbf0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e0a731db8144c1bdb30a510003f652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46847386d4b949eaa9097240a7f7ada8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.0848\n"
     ]
    }
   ],
   "source": [
    "# 2) 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.model_name_or_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\",  # GPU 있으면 GPU, 없으면 CPU\n",
    ")\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False  # 학습 시에는 False 권장\n",
    "\n",
    "# 3) LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # LLaMA 계열 기준. 다른 모델이면 target_modules 이름만 바꿔주면 됨.\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "204574dc-a614-425f-b570-4c9f2ec05f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# pad token이 없는 모델이면 eos를 pad로 재사용\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "def sft_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: list of {\"input_ids\": 1D tensor, \"attention_mask\": 1D tensor, \"labels\": 1D tensor}\n",
    "    를 받아서, padding된 배치를 반환.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids_list = [item[\"input_ids\"] for item in batch]\n",
    "    attention_mask_list = [item[\"attention_mask\"] for item in batch]\n",
    "    labels_list = [item[\"labels\"] for item in batch]\n",
    "\n",
    "    # 배치 안에서 가장 긴 길이에 맞춰 패딩\n",
    "    input_ids_padded = pad_sequence(\n",
    "        input_ids_list,\n",
    "        batch_first=True,\n",
    "        padding_value=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    attention_mask_padded = pad_sequence(\n",
    "        attention_mask_list,\n",
    "        batch_first=True,\n",
    "        padding_value=0,          # 패딩 위치는 0\n",
    "    )\n",
    "\n",
    "    labels_padded = pad_sequence(\n",
    "        labels_list,\n",
    "        batch_first=True,\n",
    "        padding_value=-100,       # loss에서 무시될 값\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_padded,\n",
    "        \"attention_mask\": attention_mask_padded,\n",
    "        \"labels\": labels_padded,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b576b00b-7f6d-458d-bf62-be1ba2dbf5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train JSONL local path: prepared_data/train.jsonl\n",
      "Total samples: 226, train: 204, val: 22\n"
     ]
    }
   ],
   "source": [
    "# S3 또는 로컬 경로를 실제 로컬 파일로 resolve\n",
    "train_jsonl_local = resolve_jsonl_path(cfg.train_jsonl)\n",
    "print(\"Train JSONL local path:\", train_jsonl_local)\n",
    "\n",
    "# Dataset 생성\n",
    "from torch.utils.data import Subset\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# 1) 전체 dataset 생성\n",
    "full_dataset = JsonlSftDataset(\n",
    "    jsonl_path=train_jsonl_local,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=cfg.max_length,\n",
    ")\n",
    "\n",
    "# 2) train / val split (예: 90% / 10%)\n",
    "n = len(full_dataset)\n",
    "val_ratio = 0.1\n",
    "val_size = max(1, int(n * val_ratio))\n",
    "\n",
    "indices = list(range(n))\n",
    "import random\n",
    "random.shuffle(indices)\n",
    "\n",
    "val_indices = indices[:val_size]\n",
    "train_indices = indices[val_size:]\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "eval_dataset = Subset(full_dataset, val_indices)\n",
    "\n",
    "print(f\"Total samples: {n}, train: {len(train_dataset)}, val: {len(eval_dataset)}\")\n",
    "\n",
    "# 3) collator (우리가 만든 labels를 그대로 유지)\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,           # 배치 내 최장 길이에 맞춰 패딩\n",
    "    # max_length=cfg.max_length,  # (선택) 강제로 이 길이까지 패딩/컷팅하고 싶으면 설정\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd5a30fd-02da-430e-91d5-fd945b2a44b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71669704-0581-48ec-a087-746bf86725d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer ready.\n"
     ]
    }
   ],
   "source": [
    "# trainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=cfg.output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    num_train_epochs=cfg.num_train_epochs,\n",
    "    per_device_train_batch_size=cfg.per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=cfg.gradient_accumulation_steps,\n",
    "    learning_rate=cfg.learning_rate,\n",
    "    warmup_ratio=cfg.warmup_ratio,\n",
    "\n",
    "    logging_steps=cfg.logging_steps,\n",
    "    save_steps=cfg.save_steps,\n",
    "    save_total_limit=3,\n",
    "\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "\n",
    "    seed=cfg.seed,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=cfg.dataloader_num_workers,\n",
    "\n",
    "    # 추가: epoch마다 eval_loss 계산\n",
    "    eval_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,   # ← 추가\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer ready.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884edab7-e28e-4c2b-9be2-04aafcb79ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(train_dataset)):\n",
    "    \n",
    "    sample = train_dataset[i]\n",
    "    print(len(sample[\"input_ids\"]), len(sample[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3fe5968-c601-4a02-88b5-9efd8c0070e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 02:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.699912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.151138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.119790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=39, training_loss=0.8509030464367989, metrics={'train_runtime': 139.1183, 'train_samples_per_second': 4.399, 'train_steps_per_second': 0.28, 'total_flos': 5516622161510400.0, 'train_loss': 0.8509030464367989, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08a3fdf0-e071-4c1f-b4a9-fee53f22bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def save_metrics_with_timestamp(df, prefix, out_dir=\"./logs\"):\n",
    "    # 1) 디렉토리 생성\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # 2) 타임스탬프 생성\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # 3) 파일 경로 생성\n",
    "    filename = f\"{prefix}_{ts}.csv\"\n",
    "    path = os.path.join(out_dir, filename)\n",
    "\n",
    "    # 4) CSV 저장\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"Saved metrics to: {path}\")\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3fd0c58-f580-4e29-84f0-0682daed4848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metrics to: ./logs/lora_metrics_20251206_031304.csv\n",
      "   epoch  eval_loss  perplexity\n",
      "0    1.0   0.699912    2.013576\n",
      "1    2.0   0.151138    1.163157\n",
      "2    3.0   0.119790    1.127260\n"
     ]
    }
   ],
   "source": [
    "#epcoh별 eval_loss와 perplexity 정리\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "rows = []\n",
    "for log in log_history:\n",
    "    # eval 단계 로그만 골라서 사용\n",
    "    if \"eval_loss\" in log:\n",
    "        epoch = log.get(\"epoch\", None)\n",
    "        eval_loss = log[\"eval_loss\"]\n",
    "        perplexity = math.exp(eval_loss)\n",
    "        rows.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"eval_loss\": eval_loss,\n",
    "            \"perplexity\": perplexity,\n",
    "        })\n",
    "\n",
    "df_metrics = pd.DataFrame(rows)\n",
    "filename = \"lora_metrics\"\n",
    "save_metrics_with_timestamp(df_metrics, filename)\n",
    "print(df_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b341d4e2-81ed-4368-ba99-29d15f294310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA fine-tuning 완료. 저장 위치: ./model\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(cfg.output_dir)      # LoRA 어댑터 포함한 모델 저장\n",
    "tokenizer.save_pretrained(cfg.output_dir)\n",
    "\n",
    "print(\"LoRA fine-tuning 완료. 저장 위치:\", cfg.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5a8500d-250e-48e7-ad75-118776fef06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9f9859fdb9448b9dadf8270495219a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "당신은 미국 주식 애널리스트다.\n",
      "\n",
      "[종목 코드] NVDA\n",
      "[기간] 2025-12-05\n",
      "\n",
      "[OHLCV 요약]\n",
      "- 시가: 370.5398\n",
      "- 고가: 372.5257\n",
      "- 저가: 367.9274\n",
      "- 종가: 367.9755\n",
      "- 거래량: 7391068.0\n",
      "\n",
      "[VQ 코드 시퀀스]\n",
      "[0.021593764424324,\t0.1893500536680221,\t0.0058695869520306,\t0.0947726070880889,\t-9.261945399163935e-28,\t0.0349617674946785,\t0.4011827707290649,\t0.5699447393417358]\n",
      "\n",
      "[뉴스 헤드라인]\n",
      "- \"나스닥 종합지수는 화요일 1.3% 상승, 기술株 주도\"\n",
      "- \"오피스 소프트웨어 기업, 다양한 제품 출시 계획 발표\"\n",
      "- \"신기술 기업, 다양한 인수합병 및 투자 유치 발표\"\n",
      "\n",
      "핵심 주제: 2025년 초 다양한 산업의 혁신 및 성장 전망. 기업들은 다양한 제품 및 서비스 출시, 인수합병 및 투자 유치 등을 통해 기업 가치 및 성장 전망을 높이고 있습니다.\n",
      "\n",
      "핵심 주제 주제: 기업의 혁신 및 성장 전망, 시장 동향 및 기업 실적. 다양한 산업의 기업들이 다양한 제품 및 서비스 출시, 인수합병 및 투자 유치 등을 통해 기업 가치 및 성장 전망을 높이고 있습니다. 기업들의 혁신 및 성장 전망은 시장 동향 및 기업 실적과 직접적으로 연관이 있습니다.\n",
      "\n",
      "핵심 주제 주제: 기업의 혁신 및 성장 전망, 시장 동향 및 기업 실적. 다양한 산업의 기업들이 다양한 제품 및 서비스 출시, 인수합병 및 투자 유치 등을 통해 기업 가치 및 성장 전망을 높이고 있습니다. 기업들의 혁\n"
     ]
    }
   ],
   "source": [
    "#simple test\n",
    "from peft import PeftModel\n",
    "\n",
    "# 베이스 모델 다시 로드 (새 세션이거나, 검증용)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "base_model.config.use_cache = True  # inference에서는 True로 켜도 됩니다.\n",
    "\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    cfg.output_dir,\n",
    ")\n",
    "lora_model.eval()\n",
    "\n",
    "def generate_comment(prompt_text: str, max_new_tokens: int = 256):\n",
    "    inputs = tokenizer(\n",
    "        prompt_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=cfg.max_length,\n",
    "    ).to(lora_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = lora_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "test_prompt = \"\"\"당신은 미국 주식 애널리스트다.\n",
    "\n",
    "[종목 코드] NVDA\n",
    "[기간] 2025-12-05\n",
    "\n",
    "[OHLCV 요약]\n",
    "- 시가: 370.5398\n",
    "- 고가: 372.5257\n",
    "- 저가: 367.9274\n",
    "- 종가: 367.9755\n",
    "- 거래량: 7391068.0\n",
    "\n",
    "[VQ 코드 시퀀스]\n",
    "[0.021593764424324,\t0.1893500536680221,\t0.0058695869520306,\t0.0947726070880889,\t-9.261945399163935e-28,\t0.0349617674946785,\t0.4011827707290649,\t0.5699447393417358]\n",
    "\n",
    "[뉴스 헤드라인]\n",
    "- \"나스닥 종합지수는 화요일 1.5% 이상 폭락한 후 수요일 1.2% 하락했습니다. S&P 500과 다우존스 산업평균지수는 모두 0.8% 하락했습니다.\"\n",
    "\n",
    "위 정보를 종합하여 300~400자 분량의 시황 코멘트를 작성하라.\n",
    "\n",
    "### 답변:\n",
    "\"\"\"\n",
    "comment = generate_comment(test_prompt)\n",
    "print(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a6e6cf-8fa0-4f3f-b37e-54301f46e5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d30d85-f471-44af-b5d7-44c20a2545c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
