{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84e7d6a7-0b31-4d69-8ce6-665eea1744c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.40.72)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.72 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3) (1.40.72)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.41.0,>=1.40.72->boto3) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.41.0,>=1.40.72->boto3) (1.26.20)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.72->boto3) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers peft accelerate datasets boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8504bcfb-7b3d-4bf9-a494-2e7f94570429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c22921ce-9380-4836-8a1f-6758da326b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainConfig(model_name_or_path='meta-llama/Meta-Llama-3-8B-Instruct', train_jsonl='s3://my-bucket/path/to/train.jsonl', output_dir='./model', max_length=1024, per_device_train_batch_size=1, gradient_accumulation_steps=16, num_train_epochs=3, learning_rate=0.0002, warmup_ratio=0.03, logging_steps=50, save_steps=500, seed=42, dataloader_num_workers=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    # 1) 모델 & 데이터\n",
    "    model_name_or_path: str = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # 원하는 베이스 LLM 이름\n",
    "    train_jsonl: str = \"prepared_data/train.json\"          # 학습시킬 json prompt\n",
    "    \n",
    "    # 결과 저장 위치: EBS 볼륨 내 현재 작업 디렉토리 기준 \"./model\"\n",
    "    output_dir: str = \"./model\"\n",
    "\n",
    "    # 2) 토크나이즈 & 길이\n",
    "    max_length: int = 1024\n",
    "\n",
    "    # 3) 학습 하이퍼파라미터\n",
    "    per_device_train_batch_size: int = 1\n",
    "    gradient_accumulation_steps: int = 16\n",
    "    num_train_epochs: int = 3\n",
    "    learning_rate: float = 2e-4\n",
    "    warmup_ratio: float = 0.03\n",
    "\n",
    "    # 4) 로깅 & 저장\n",
    "    logging_steps: int = 50\n",
    "    save_steps: int = 500\n",
    "    seed: int = 42\n",
    "    # 5) 기타\n",
    "    dataloader_num_workers: int = 2  # Colab이면 0~2 정도로 유지\n",
    "\n",
    "\n",
    "cfg = TrainConfig()\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f9225a-5329-42df-9750-386224287c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_jsonl_path(path: str) -> str:\n",
    "    \"\"\"\n",
    "    - s3://bucket/key 형식이면 /tmp/train.jsonl 로 다운로드 후 해당 경로 반환\n",
    "    - 그 외에는 로컬 경로 그대로 반환\n",
    "    \"\"\"\n",
    "    if path.startswith(\"s3://\"):\n",
    "        no_scheme = path[5:]\n",
    "        bucket, key = no_scheme.split(\"/\", 1)\n",
    "\n",
    "        local_path = \"/tmp/train.jsonl\"\n",
    "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "\n",
    "        s3 = boto3.client(\"s3\")\n",
    "        s3.download_file(bucket, key, local_path)\n",
    "        print(f\"[INFO] downloaded {path} -> {local_path}\")\n",
    "        return local_path\n",
    "    else:\n",
    "        return path\n",
    "\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "seed_everything(cfg.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4ead72-8cc8-425f-92cb-5ddb5bd09d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonlSftDataset(Dataset):\n",
    "    \"\"\"\n",
    "    JSONL SFT 포맷:\n",
    "      {\"prompt\": \"...\", \"completion\": \"...\", \"date\": \"...\"} 한 줄씩\n",
    "\n",
    "    full_text = prompt_text + \"\\\\n\\\\n### 답변:\\\\n\" + completion\n",
    "    을 만들고, prompt_text + \"### 답변:\" 구간까지는 labels를 -100으로 마스킹.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, jsonl_path: str, tokenizer, max_length: int = 1024):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.records: List[Dict] = []\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                obj = json.loads(line)\n",
    "                prompt = obj[\"prompt\"]\n",
    "                completion = obj[\"completion\"]\n",
    "                self.records.append(\n",
    "                    {\n",
    "                        \"prompt\": prompt,\n",
    "                        \"completion\": completion,\n",
    "                        \"date\": obj.get(\"date\", None),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        rec = self.records[idx]\n",
    "        prompt = rec[\"prompt\"]\n",
    "        completion = rec[\"completion\"]\n",
    "\n",
    "        # 1) 프롬프트 텍스트 / 전체 텍스트 구성\n",
    "        #    여기까지가 \"입력\" 역할\n",
    "        prompt_text = prompt.rstrip() + \"\\n\\n### 답변:\\n\"\n",
    "        full_text = prompt_text + completion.rstrip()\n",
    "\n",
    "        # 2) prompt 길이(토큰 개수) 계산 - special tokens 제외\n",
    "        prompt_ids = self.tokenizer(\n",
    "            prompt_text,\n",
    "            add_special_tokens=False,\n",
    "        )[\"input_ids\"]\n",
    "        prompt_len = len(prompt_ids)\n",
    "\n",
    "        # 3) full_text 토크나이즈 (special tokens 포함)\n",
    "        encoded = self.tokenizer(\n",
    "            full_text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "\n",
    "        input_ids = encoded[\"input_ids\"]\n",
    "        attn_mask = encoded[\"attention_mask\"]\n",
    "\n",
    "        # 4) labels = input_ids 복사 후, prompt 부분 마스킹\n",
    "        labels = input_ids.copy()\n",
    "\n",
    "        # BOS 토큰이 앞에 붙어 있는 경우, 그만큼 추가로 마스킹\n",
    "        bos_extra = 0\n",
    "        if hasattr(self.tokenizer, \"bos_token_id\") and self.tokenizer.bos_token_id is not None:\n",
    "            if len(input_ids) > 0 and input_ids[0] == self.tokenizer.bos_token_id:\n",
    "                bos_extra = 1\n",
    "\n",
    "        mask_len = min(len(labels), prompt_len + bos_extra)\n",
    "        for i in range(mask_len):\n",
    "            labels[i] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attn_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094d7b3b-5455-47b4-8b1b-ae8910bee621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 토크나이저\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    cfg.model_name_or_path,\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "# pad 토큰 세팅 (없는 모델들 대비)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# 2) 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",  # GPU 있으면 GPU, 없으면 CPU\n",
    ")\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False  # 학습 시에는 False 권장\n",
    "\n",
    "# 3) LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # LLaMA 계열 기준. 다른 모델이면 target_modules 이름만 바꿔주면 됨.\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b576b00b-7f6d-458d-bf62-be1ba2dbf5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 또는 로컬 경로를 실제 로컬 파일로 resolve\n",
    "train_jsonl_local = resolve_jsonl_path(cfg.train_jsonl)\n",
    "print(\"Train JSONL local path:\", train_jsonl_local)\n",
    "\n",
    "# Dataset 생성\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# 1) 전체 dataset 생성\n",
    "full_dataset = JsonlSftDataset(\n",
    "    jsonl_path=train_jsonl_local,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=cfg.max_length,\n",
    ")\n",
    "\n",
    "# 2) train / val split (예: 90% / 10%)\n",
    "n = len(full_dataset)\n",
    "val_ratio = 0.1\n",
    "val_size = max(1, int(n * val_ratio))\n",
    "\n",
    "indices = list(range(n))\n",
    "import random\n",
    "random.shuffle(indices)\n",
    "\n",
    "val_indices = indices[:val_size]\n",
    "train_indices = indices[val_size:]\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "eval_dataset = Subset(full_dataset, val_indices)\n",
    "\n",
    "print(f\"Total samples: {n}, train: {len(train_dataset)}, val: {len(eval_dataset)}\")\n",
    "\n",
    "# 3) collator (우리가 만든 labels를 그대로 유지)\n",
    "data_collator = default_data_collator\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71669704-0581-48ec-a087-746bf86725d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=cfg.output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    num_train_epochs=cfg.num_train_epochs,\n",
    "    per_device_train_batch_size=cfg.per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=cfg.gradient_accumulation_steps,\n",
    "    learning_rate=cfg.learning_rate,\n",
    "    warmup_ratio=cfg.warmup_ratio,\n",
    "\n",
    "    logging_steps=cfg.logging_steps,\n",
    "    save_steps=cfg.save_steps,\n",
    "    save_total_limit=3,\n",
    "\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "\n",
    "    seed=cfg.seed,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=cfg.dataloader_num_workers,\n",
    "\n",
    "    # 추가: epoch마다 eval_loss 계산\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,   # ← 추가\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer ready.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fe5968-c601-4a02-88b5-9efd8c0070e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3fdf0-e071-4c1f-b4a9-fee53f22bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def save_metrics_with_timestamp(df, prefix = \"lora_metrics\", out_dir=\"./logs\"):\n",
    "    # 1) 디렉토리 생성\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fd0c58-f580-4e29-84f0-0682daed4848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#epcoh별 eval_loss와 perplexity 정리\n",
    "import math\n",
    "\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "rows = []\n",
    "for log in log_history:\n",
    "    # eval 단계 로그만 골라서 사용\n",
    "    if \"eval_loss\" in log:\n",
    "        epoch = log.get(\"epoch\", None)\n",
    "        eval_loss = log[\"eval_loss\"]\n",
    "        perplexity = math.exp(eval_loss)\n",
    "        rows.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"eval_loss\": eval_loss,\n",
    "            \"perplexity\": perplexity,\n",
    "        })\n",
    "\n",
    "df_metrics = pd.DataFrame(rows)\n",
    "save_metrics_with_timestamp(df_metrics)\n",
    "print(df_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b341d4e2-81ed-4368-ba99-29d15f294310",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(cfg.output_dir)      # LoRA 어댑터 포함한 모델 저장\n",
    "tokenizer.save_pretrained(cfg.output_dir)\n",
    "\n",
    "print(\"LoRA fine-tuning 완료. 저장 위치:\", cfg.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5a8500d-250e-48e7-ad75-118776fef06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple test\n",
    "from peft import PeftModel\n",
    "\n",
    "# 베이스 모델 다시 로드 (새 세션이거나, 검증용)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "base_model.config.use_cache = True  # inference에서는 True로 켜도 됩니다.\n",
    "\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    cfg.output_dir,\n",
    ")\n",
    "lora_model.eval()\n",
    "\n",
    "def generate_comment(prompt_text: str, max_new_tokens: int = 256):\n",
    "    inputs = tokenizer(\n",
    "        prompt_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=cfg.max_length,\n",
    "    ).to(lora_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = lora_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "test_prompt = \"\"\"당신은 한국 주식 애널리스트다.\n",
    "\n",
    "[종목] 005930\n",
    "[기간] 2025-12-05\n",
    "\n",
    "[OHLCV 요약]\n",
    "- 시가: ...\n",
    "- 고가: ...\n",
    "- 저가: ...\n",
    "- 종가: ...\n",
    "- 거래량: ...\n",
    "\n",
    "[VQ 코드 시퀀스]\n",
    "[12, 45, 78, 90]\n",
    "\n",
    "[뉴스 헤드라인]\n",
    "- \"삼성전자, XXX 관련 수주 기대\"\n",
    "\n",
    "위 정보를 종합하여 2~3문장 분량의 시황 코멘트를 작성하라.\n",
    "\n",
    "### 답변:\n",
    "\"\"\"\n",
    "\n",
    "print(generate_comment(test_prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a6e6cf-8fa0-4f3f-b37e-54301f46e5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
