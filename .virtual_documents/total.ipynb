get_ipython().getoutput("pip install --upgrade pip wheel setuptools")

# Go ÏóÜÏù¥ ÏÑ§Ïπò Í∞ÄÎä•Ìïú ÏòàÏ†Ñ Î≤ÑÏ†Ñ ÏÇ¨Ïö©
get_ipython().getoutput("pip install "wandb<0.23"")
get_ipython().getoutput("pip install numpy")
get_ipython().getoutput("pip install pandas")
get_ipython().getoutput("pip install torch")
get_ipython().getoutput("pip install scikit-learn")
get_ipython().getoutput("pip install tqdm")
get_ipython().getoutput("pip install google")
get_ipython().getoutput("pip install google-genai pandas --upgrade")
get_ipython().getoutput("pip install s3fs")
get_ipython().getoutput("pip install numpy")









import os
import wandb

import numpy as np
import pandas as pd

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset
from torch.utils.data import DataLoader, random_split

from sklearn.preprocessing import MinMaxScaler

from tqdm import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"

print(device)


# from google.colab import drive
# drive.mount('/content/drive')


csv_filename = 'spy_2023_2024.csv'
csv_filepath = '/content/drive/MyDrive/2025 ML Project/datasets/spy_data.csv'

save_dir = 'prepared_data/'


#wandb.finish()


#wandb.login()






# csv to DF
data = pd.read_csv(csv_filename)
data["date"] = pd.to_datetime(data["date"])


data


## ÌïòÎ£®Î•º Í∏∞Ï§ÄÏúºÎ°ú Ï†ïÍ∑úÌôî
def normalize_per_day(group):
  ohlc = ['1. open', '2. high', '3. low', '4. close']

  min_val = group[ohlc].min().min()
  max_val = group[ohlc].max().max()

  if max_val - min_val > 0:
    group[ohlc] = (group[ohlc] - min_val) / (max_val - min_val)
  else:
    group[ohlc] = 0.5

  ## ÏùºÎã® volumeÎèÑ ÌïòÎ£® Îã®ÏúÑÎ°ú Ï†ïÍ∑úÌôî
  min_vol = group['5. volume'].min()
  max_vol = group['5. volume'].max()
  group['5. volume'] = (group['5. volume'] - min_vol) / (max_vol - min_vol)

  return group

normalized_data = data.groupby(data['date'].dt.date).apply(normalize_per_day)


normalized_data


normalized_data.head(14)


class SPYDataSet(Dataset):
  def __init__(self, data, features, chunk_size):
    self.chunk_size = chunk_size

    arr = data[features].to_numpy(dtype=np.float32)
    self.arr = arr
    self.N, self.C = arr.shape

    self.num_chunks = self.N // self.chunk_size

  def __len__(self):
    return self.num_chunks

  def __getitem__(self, idx: int):
    start = idx * self.chunk_size
    end = start + self.chunk_size

    x = self.arr[start:end]
    x = torch.from_numpy(x).float().T
    return {"x": x, "idx": idx}





feature_cols = ['1. open', '2. high', '3. low', '4. close', '5. volume']

ds = SPYDataSet(normalized_data, features=feature_cols, chunk_size=13)
print(len(ds))


ds[0] # C(5) * T(13)





class Encoder(nn.Module):
  def __init__(self, input_dim, hidden_dim, latent_dim):
    super().__init__()
    self.conv = nn.Sequential(
        nn.Conv1d(input_dim, hidden_dim, kernel_size=5, stride=2, padding=2),
        nn.ReLU(),
        nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, stride=1, padding=2),
        nn.ReLU(),
        nn.Conv1d(hidden_dim, latent_dim, kernel_size=3, stride=1, padding=1),
        nn.ReLU(),
        nn.AdaptiveAvgPool1d(1)
    )

  def forward(self, x):
    return self.conv(x)


class Decoder(nn.Module):
  def __init__(self, latent_dim, hidden_dim, output_dim):
    super().__init__()
    self.deconv = nn.Sequential(
          nn.ConvTranspose1d(latent_dim, hidden_dim, kernel_size=7, stride=1, padding=0),
          nn.ReLU(),
          nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1),
          nn.ReLU(),
          nn.ConvTranspose1d(hidden_dim, hidden_dim, kernel_size=3, stride=2, padding=1, output_padding=0),
          nn.ReLU(),
          nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1),
          nn.ReLU(),
          nn.Conv1d(hidden_dim, output_dim, kernel_size=3, stride=1, padding=1)
      )

  def forward(self, z_q):
      return self.deconv(z_q)


class VectorQuantizer(nn.Module):
  """ num_embeddings: K (codebook size)
      embedding_dim:  D (code dimension)
      commitment_cost: beta in the paper """

  def __init__(self, num_embeddings, embedding_dim, commitment_cost):
    super().__init__()
    self.num_embeddings = num_embeddings
    self.embedding = nn.Embedding(num_embeddings, embedding_dim)
    self.commitment_cost = commitment_cost
    self.embedding.weight.data.uniform_(-1/num_embeddings, 1/num_embeddings)

  def forward(self, z):
    B, D, T = z.shape
    z_perm = z.permute(0, 2, 1).contiguous()
    z_flattened = z_perm.view(-1, D)

    e = self.embedding.weight
    z_sq = (z_flattened ** 2).sum(dim=1, keepdim=True)
    e_sq = (e ** 2).sum(dim=1)
    ze = z_flattened @ e.t()
    distances = z_sq + e_sq.unsqueeze(0) - 2 * ze

    encoding_indices = torch.argmin(distances, dim=1)
    z_q = self.embedding(encoding_indices).view(B, T, D).permute(0, 2, 1).contiguous()

    codebook_loss =  F.mse_loss(z_q, z.detach())
    commitment_loss = self.commitment_cost * F.mse_loss(z_q.detach(), z)
    vq_loss = codebook_loss + 0.5 * commitment_loss

    z_q = z + (z_q - z).detach()

    indices_bt = encoding_indices.view(B, T)
    return z_q, vq_loss, indices_bt


class VQVAE(nn.Module):
  def __init__(self, input_dim, hidden_dim, latent_dim, num_embeddings, commitment_cost):
    super().__init__()
    self.encoder = Encoder(input_dim, hidden_dim, latent_dim)
    self.vq = VectorQuantizer(num_embeddings, latent_dim, commitment_cost)
    self.decoder = Decoder(latent_dim, hidden_dim, input_dim)

  def forward(self, x):
    z_e = self.encoder(x)
    z_q, vq_loss, indices = self.vq(z_e)
    x_recon = self.decoder(z_q)
    return x_recon, vq_loss, indices, z_q


def evaluate(model, dataloader, device):
  index_list = []
  sum_recon, sum_vq = 0.0, 0.0
  n = 0

  model.eval()
  with torch.no_grad():
    for batch in tqdm(dataloader, desc="Evaluating", leave=False):
      X = batch["x"].to(device)

      x_recon, vq_loss, indices, _ = model(X)

      recon_loss = F.mse_loss(x_recon, X)

      batch_size = X.size(0)
      sum_recon += recon_loss.item() * batch_size
      sum_vq += vq_loss.item() * batch_size
      n += batch_size

      index_list.append(indices)

    mean_recon = sum_recon / max(n, 1)
    mean_vq = sum_vq / max(n, 1)
    sum_loss = mean_recon + mean_vq

    return mean_recon, mean_vq, sum_loss, index_list


import wandb
print(wandb.__file__)


configs= [{
        "batch_size": 32,
        "learning_rate": 3e-4,
        "epochs": 200,
        "num_embeddings": 16,
        "commitment_cost":0.25
    }, {
        "batch_size": 32,
        "learning_rate": 3e-4,
        "epochs": 200,
        "num_embeddings": 16,
        "commitment_cost":0.4
    }, {
        "batch_size": 32,
        "learning_rate": 3e-4,
        "epochs": 200,
        "num_embeddings": 32,
        "commitment_cost":0.25
    }, {
        "batch_size": 32,
        "learning_rate": 3e-4,
        "epochs": 200,
        "num_embeddings": 32,
        "commitment_cost":0.4
    }, {
        "batch_size": 32,
        "learning_rate": 3e-4,
        "epochs": 200,
        "num_embeddings": 64,
        "commitment_cost":0.25
    }, {
        "batch_size": 32,
        "learning_rate": 3e-4,
        "epochs": 200,
        "num_embeddings": 64,
        "commitment_cost":0.4
    }]



for i in range(len(configs)):
    cfg = configs[i]
    wandb.init(
        project="2025 ML Project",
        mode="offline",
        entity="youani-korea-university",
        name="1 vector_8",
        config= cfg
    )
    
    N = len(ds)
    train_len = int(N * 0.7)
    val_len = int(N * 0.15)
    test_len = N - train_len - val_len
    
    ds_train, ds_val, ds_test = random_split(ds, [train_len, val_len, test_len])
    train_loader = DataLoader(ds_train, batch_size=wandb.config.batch_size, shuffle=True, drop_last=True)
    val_loader   = DataLoader(ds_val, batch_size=wandb.config.batch_size, shuffle=False)
    test_loader  = DataLoader(ds_test, batch_size=wandb.config.batch_size, shuffle=False)
    
    # parameter setting
    input_dim = 5
    hidden_dim = 64
    latent_dim = 8
    num_embeddings = wandb.config.num_embeddings
    commitment_cost = wandb.config.commitment_cost
    
    # model training
    lr = wandb.config.learning_rate
    epochs = wandb.config.epochs
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = VQVAE(input_dim, hidden_dim, latent_dim, num_embeddings, commitment_cost)
    model.to(device)
    opt = torch.optim.Adam(model.parameters(), lr=lr)
    recon_hist, vq_hist = [], []
    
    best_val_loss = float("inf")
    best_model_path = 'prepared_data/best_model_epoch.pt'
    
    for epoch in range(1, epochs + 1):
      model.train()
      sum_recon, sum_vq = 0.0, 0.0
      n = 0
    
      for batch in tqdm(train_loader, desc="Train", leave=False):
        X = batch['x'].to(device)
        opt.zero_grad()
        x_recon, vq_loss, indices, _ = model(X)
    
        if x_recon.size(-1) != X.size(-1):
          print("Error: reconstruction size not equal to original")
          x_recon = F.interpolate(x_recon, size=X.size(-1), mode='linear', align_corners=False)
    
        recon_loss = F.mse_loss(x_recon, X)
        loss = recon_loss + vq_loss
    
        loss.backward()
        opt.step()
    
        batch_size = X.size(0)
        sum_recon += recon_loss.item() * batch_size
        sum_vq += vq_loss.item() * batch_size
        n += batch_size
    
      epoch_recon = sum_recon / max(n, 1)
      epoch_vq = sum_vq / max(n, 1)
    
      recon_hist.append(epoch_recon)
      vq_hist.append(epoch_vq)
    
      tqdm.write(f"[{epoch:03d}/{epochs:03d} Training] recon={epoch_recon:.6f} vq={epoch_vq:.6f}")
    
      val_recon, val_vq, val_loss, index_list = evaluate(model, val_loader, device)
      index_total = torch.cat(index_list).view(-1)
      usage_rate = len(index_total.unique()) / model.vq.num_embeddings
      wandb.log({
          f'valid_epoch_recon': val_recon,
          f'valid_epoch_vq': val_vq,
          f'usage_rate': usage_rate
      })
      tqdm.write(f"[{epoch:03d}/{epochs:03d} Validation] recon={val_recon:.6f} vq={val_vq:.6f}")
    
      if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_model_path = os.path.join("model/vqvae/",f"num_embedding_{cfg['num_embeddings']}_commitment_cost_{cfg['commitment_cost']}.pt")
        torch.save(model.state_dict(),best_model_path)
    
    print(f"Training Finished. Best model: {best_model_path} (val_loss: {best_val_loss:.6f})")


model.load_state_dict(torch.load(best_model_path))

test_recon, test_vq, test_loss, index_list = evaluate(model, test_loader, device)
wandb.log({
    f'test_epoch_recon': test_recon,
    f'test_epoch_vq': test_vq
})
print(f"[Test] recon={test_recon:.6f} vq={test_vq:.6f}")


wandb.finish()





all_x = []
all_assign = []

model.eval()
with torch.no_grad():
  for batch in test_loader:
    X = batch["x"].to(device)
    _, _, indices, _ = model(X)

    all_x.append(X.cpu())
    all_assign.append(indices.cpu())

X = torch.cat(all_x, dim=0)
assign = torch.cat(all_assign, dim=0)
assign


## Embedding Extract


wandb.init()


from tqdm import tqdm

# Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏö© DataLoader (shuffle=False ÌïÑÏàò)
full_loader = DataLoader(
    ds,
    batch_size=wandb.config.batch_size,
    shuffle=False
)



model.eval()

all_idx = []
all_codes = []
all_embs = []  # z_q Î≤°ÌÑ∞ (ÏòµÏÖò)

with torch.no_grad():
    for batch in tqdm(full_loader, desc="Extract VQ codes"):
        X = batch["x"].to(device)          # (B, C, T)
        idxs = batch["idx"]                # Ï†ÑÏó≠ ÏúàÎèÑÏö∞ index

        x_recon, vq_loss, indices_bt, z_q = model(X)
        # indices_bt: (B, T=1) ‚Üí Í∞Å ÏúàÎèÑÏö∞Îãπ ÌïòÎÇòÏùò ÏΩîÎìú
        codes = indices_bt.squeeze(1).cpu().numpy()   # (B,)
        z_vec = z_q.squeeze(-1).cpu().numpy()         # (B, latent_dim)

        all_idx.extend(idxs.cpu().numpy().tolist())
        all_codes.extend(codes.tolist())
        all_embs.extend(z_vec.tolist())



print(ds[1])


import os

chunk_size = ds.chunk_size
rows = []
for idx, code, emb in zip(all_idx, all_codes, all_embs):
    start = idx * chunk_size
    end = start + chunk_size
    ts = normalized_data.iloc[end - 1]["date"]
    ts = ts.date()

    row = {"date": ts, "code": int(code)}
    for j, v in enumerate(emb):
        row[f"z_{j}"] = float(v)
    rows.append(row)

df_vq = pd.DataFrame(rows).sort_values("date").reset_index(drop=True)

# 2) Î°úÏª¨ ÏûÑÏãú ÌååÏùº Í≤ΩÎ°ú
local_path = "prepared_data/embedding.csv"
df_vq.to_csv(local_path, index=False, encoding="utf-8-sig")



print(ds[1])





import pandas as pd
import json


embedding_path = "prepared_data/embedding.csv"
embedding_df = pd.read_csv(embedding_path)
embedding_df["date"] = pd.to_datetime(embedding_df["date"]).dt.date
embedding_df


news_path = "processed_news.csv"
news_df = pd.read_csv(news_path)
news_df["date"] = pd.to_datetime(news_df["date"]).dt.date
news_df


group_news_df = news_df.groupby("date")["text"].apply(list).reset_index()
group_news_df


group_news_df["text"] = group_news_df["text"].apply(json.dumps)
group_news_df


final_df = embedding_df.merge(group_news_df, on="date", how="inner")


final_df


final_path = "prepared_data/final.csv"
final_df.to_csv(final_path, index=False)





import pandas as pd
from google import genai
from google.genai.errors import APIError
import time

# --- 1. ÏÑ§Ï†ï Î∞è Ï¥àÍ∏∞Ìôî ---

# ‚ö†Ô∏è Ïó¨Í∏∞Ïóê Ïã§Ï†ú Gemini API ÌÇ§Î•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî.
# ÌôòÍ≤Ω Î≥ÄÏàòÎ•º ÏÇ¨Ïö©ÌïòÎäî Í≤ÉÏùÑ Í∂åÏû•ÌïòÏßÄÎßå, ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ ÏßÅÏ†ë ÏûÖÎ†•Ìï† ÏàòÎèÑ ÏûàÏäµÎãàÎã§.
API_KEY = ""
try:
    # client Í∞ùÏ≤¥Îäî for Î£®ÌîÑ Î∞ñÏóêÏÑú Ìïú Î≤àÎßå Ï¥àÍ∏∞ÌôîÎê©ÎãàÎã§.
    client = genai.Client(api_key=API_KEY)
    model = 'gemini-2.5-flash'  # Îπ†Î•∏ ÏùëÎãµÏùÑ ÏúÑÌï¥ flash Î™®Îç∏ ÏÇ¨Ïö©
except Exception as e:
    print(f"Gemini ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî Ïò§Î•ò: {e}")
    exit()

# --- 2. Îç∞Ïù¥ÌÑ∞ Î°úÎìú ---

file_path = 'prepared_data/final.csv'
try:
    df = pd.read_csv(file_path)
    print(f"‚úÖ ÌååÏùº '{file_path}' Î°úÎìú ÏôÑÎ£å. Ï¥ù {len(df)}Í∞ú Ìñâ.")
except FileNotFoundError:
    print(f"‚ùå Ïò§Î•ò: ÌååÏùº '{file_path}'Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. Í≤ΩÎ°úÎ•º ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî.")
    exit()
except Exception as e:
    print(f"‚ùå Ïò§Î•ò: CSV ÌååÏùºÏùÑ ÏùΩÎäî Ï§ë Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {e}")
    exit()

# Í∑∏Î£πÌôîÌï† ÌÇ§ Ïª¨Îüº
GROUPING_KEY = 'code'

if GROUPING_KEY not in df.columns:
    print(f"‚ùå Ïò§Î•ò: Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏóê Í∑∏Î£πÌôî ÌÇ§Ïù∏ '{GROUPING_KEY}' Ïó¥Ïù¥ ÏóÜÏäµÎãàÎã§.")
    exit()

# --- 3. Silver Label ÏÉùÏÑ±ÏùÑ ÏúÑÌïú Ìï®Ïàò Ï†ïÏùò ---

def generate_silver_label(text_list, code_value, client_obj, model_name, prompt_input):
    """
    Ï£ºÏñ¥ÏßÑ ÌÖçÏä§Ìä∏ Î¶¨Ïä§Ìä∏Î•º Gemini APIÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÏöîÏïΩÌï©ÎãàÎã§.
    (API ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Í∞ùÏ≤¥Î•º Ïù∏ÏàòÎ°ú Î∞õÎèÑÎ°ù ÏàòÏ†ï)
    """
    if not text_list:
        return ""

    # ÌÖçÏä§Ìä∏Îì§ÏùÑ ÌïòÎÇòÏùò Î¨∏ÏûêÏó¥Î°ú Í≤∞Ìï©
    combined_text = "\n---\n".join(text_list)
    combined_text = combined_text[:100000]
    # ÌîÑÎ°¨ÌîÑÌä∏ Ï†ïÏùò
    prompt = f"""
    ÏïÑÎûòÏóê Ï†úÍ≥µÎêú ÌÖçÏä§Ìä∏Îì§ÏùÄ Í≥µÌÜµÏ†ÅÏúºÎ°ú ÏΩîÎìú Í∞í {code_value}Î•º Í∞ÄÏßÄÎäî Îâ¥Ïä§ Í∏∞ÏÇ¨/Ï†ïÎ≥¥Ïùò ÏùºÎ∂ÄÏûÖÎãàÎã§.
    
    Ïù¥ Î™®Îì† ÌÖçÏä§Ìä∏Ïùò ÌïµÏã¨ ÎÇ¥Ïö©Í≥º Ï£ºÏ†úÎ•º ÌååÏïÖÌïòÏó¨, 100~200Ïûê ÏÇ¨Ïù¥Ïùò ÌïòÎÇòÏùò **ÌïúÍµ≠Ïñ¥ ÏöîÏïΩ(Silver Label)**ÏúºÎ°ú Ï†ïÎ¶¨Ìï¥ Ï£ºÏÑ∏Ïöî.
    ÏöîÏïΩÏùÄ Îã§Ïùå ÌòïÏãùÏùÑ Îî∞Î¶ÖÎãàÎã§: "ÌïµÏã¨ Ï£ºÏ†ú: (ÌïµÏã¨ ÎÇ¥Ïö©)"
    
    --- ÌÖçÏä§Ìä∏ Î™©Î°ù ---
    {combined_text}
    """

    try:
        # Gemini API Ìò∏Ï∂ú
        response = client_obj.models.generate_content(
            model=model_name,
            contents=prompt
        )
        return response.text.strip()
    
    # ÌÜ†ÌÅ∞ ÎßåÎ£å ÎòêÎäî Í∏∞ÌÉÄ API Ïò§Î•ò Î∞úÏÉù Ïãú
    except APIError as e:
        # Ïò§Î•ò ÏÉÅÏÑ∏ Ï†ïÎ≥¥Î•º Î∞òÌôòÌïòÏó¨ ÎÇòÏ§ëÏóê ÏõêÏù∏ÏùÑ Î∂ÑÏÑùÌï† Ïàò ÏûàÍ≤å Ìï®
        return f"ÏöîÏïΩ Ïò§Î•ò: API Error - {e}"
    except Exception as e:
        return f"ÏöîÏïΩ Ïò§Î•ò: Unexpected Error - {e}"

# --- 4. Í∑∏Î£πÎ≥Ñ ÏöîÏïΩ Î∞è Silver Label ÏÉùÏÑ± (ÏßÄÏó∞/Î∞òÎ≥µÎ¨∏ Ï†ÅÏö©) ---

# GROUPING_KEYÎ•º Í∏∞Ï§ÄÏúºÎ°ú Í∑∏Î£πÌôî
grouped = df.groupby(GROUPING_KEY)['text'].apply(list).reset_index(name='text_list')
print(f"‚öôÔ∏è {GROUPING_KEY} Í∞í {len(grouped)}Í∞úÎ°ú Í∑∏Î£πÌôî ÏôÑÎ£å.")

# 'silver_label' Ïó¥ÏùÑ ÎØ∏Î¶¨ Ï¥àÍ∏∞ÌôîÌï©ÎãàÎã§.
grouped['silver_label'] = None 

print("‚ú® Í∑∏Î£πÎ≥Ñ Silver Label ÏÉùÏÑ± Ï§ë... (API Ìò∏Ï∂ú ÏÇ¨Ïù¥Ïóê **1Î∂Ñ ÏßÄÏó∞**Ïù¥ Ï†ÅÏö©Îê©ÎãàÎã§.)")

# Ïù∏Îç±Ïä§Î•º Î¶¨Ïä§Ìä∏Î°ú Î≥ÄÌôòÌïòÏó¨ ÎßàÏßÄÎßâ Ïù∏Îç±Ïä§Î•º ÏâΩÍ≤å ÌôïÏù∏
group_indices = grouped.index.tolist()
total_groups = len(group_indices)


prompts = [
  {
    "prompt_number": 1,
    "prompt_context": "ÏÉÅÏÑ∏ Î∂ÑÏÑù Î∞è Ìï≠Î™© Í∞ïÏ†ú: ÏöîÏïΩÏóê Î∞òÎìúÏãú 'Ï£ºÏöî ÏÇ∞ÏóÖ Î∂ÑÏïº/Í∏∞Ïà† Ìä∏Î†åÎìú', 'Ï£ºÏöî Ïù¥Ïäà/ÌôúÎèô Ïú†Ìòï', 'Ï†ÑÎ∞òÏ†ÅÏù∏ ÏãúÏû• Î∂ÑÏúÑÍ∏∞/ÏòÅÌñ•' ÏÑ∏ Í∞ÄÏßÄ ÏöîÏÜåÎ•º Ìè¨Ìï®ÌïòÎèÑÎ°ù Í∞ïÏ†úÌïòÏó¨ Íµ¨Ï≤¥Ï†ÅÏù∏ Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞Î•º ÌôïÎ≥¥Ìï©ÎãàÎã§.",
    "prompt_text": "ÏïÑÎûò Ï†úÍ≥µÎêú ÌÖçÏä§Ìä∏Îì§ÏùÄ Í≥µÌÜµÏ†ÅÏúºÎ°ú ÏΩîÎìú Í∞í {code_value}Î•º Í∞ÄÏßÄÎäî Îâ¥Ïä§ Í∏∞ÏÇ¨/Ï†ïÎ≥¥Ïùò ÏùºÎ∂ÄÏûÖÎãàÎã§.\n\nÎãπÏã†ÏùÄ Ï†ÑÎ¨∏ Îâ¥Ïä§ Î∂ÑÏÑùÍ∞ÄÏûÖÎãàÎã§. Ïù¥ Î™®Îì† ÌÖçÏä§Ìä∏Î•º Î∂ÑÏÑùÌïòÏó¨, Îã§Ïùå ÏÑ∏ Í∞ÄÏßÄ ÌïµÏã¨ Ìï≠Î™©ÏùÑ Î∞òÎìúÏãú Ìè¨Ìï®ÌïòÎäî 100~200Ïûê ÏÇ¨Ïù¥Ïùò ÌïòÎÇòÏùò **ÌïúÍµ≠Ïñ¥ ÏöîÏïΩ(Silver Label)**ÏúºÎ°ú Ï†ïÎ¶¨Ìï¥ Ï£ºÏÑ∏Ïöî.\n\n1. **Ïñ∏Í∏âÎêú Ï£ºÏöî ÏÇ∞ÏóÖ Î∂ÑÏïº ÎòêÎäî Í∏∞Ïà† Ìä∏Î†åÎìú** (Ïòà: IT, Ìó¨Ïä§ÏºÄÏñ¥, ESG, AI Îì±)\n2. **Ï£ºÏöî Ïù¥Ïäà/ÌôúÎèôÏùò Ïú†Ìòï** (Ïòà: Ïù∏ÏàòÌï©Î≥ë, Ïã†Ï†úÌíà Ï∂úÏãú, Ïã§Ï†Å Î∞úÌëú, Ï†ïÏ±Ö Î≥ÄÌôî Îì±)\n3. **Ï†ÑÎ∞òÏ†ÅÏù∏ ÏãúÏû• Î∂ÑÏúÑÍ∏∞ ÎòêÎäî ÏòÅÌñ•** (Ïòà: ÏÑ±Ïû• Í∏∞ÎåÄ, Î∂àÌôïÏã§ÏÑ± Ï¶ùÎåÄ, Í∑úÏ†ú Í∞ïÌôî Îì±)\n\nÏöîÏïΩÏùÄ Î∞òÎìúÏãú Îã§Ïùå ÌòïÏãùÏùÑ Îî∞Î•¥Î©∞, Íµ¨Ï≤¥Ï†ÅÏù¥Ïñ¥Ïïº Ìï©ÎãàÎã§:\n\"ÌïµÏã¨ Ï£ºÏ†ú: (ÌïµÏã¨ ÎÇ¥Ïö©. ÏúÑ 3Í∞ÄÏßÄ Ìï≠Î™©Ïù¥ Î™®Îëê ÎÖπÏïÑ ÏûàÏñ¥Ïïº Ìï®)\"\n\n--- ÌÖçÏä§Ìä∏ Î™©Î°ù ---\n{combined_text}",
    "output_name": "silver_label_detailed_analysis.csv"
  },
  {
    "prompt_number": 2,
    "prompt_context": "ÏãúÏû• ÎèôÌñ• Ï§ëÏã¨ ÏöîÏïΩ: Í∞úÎ≥Ñ Í∏∞ÏóÖ ÌôúÎèôÎ≥¥Îã§Îäî Í±∞ÏãúÏ†Å Í¥ÄÏ†êÏùò 'ÏãúÏû• Î≥ÄÌôî'ÏôÄ 'ÏúÑÌóò ÏöîÏÜå'Ïóê Ï¥àÏ†êÏùÑ ÎßûÏ∂∞ ÏöîÏïΩÌïòÎèÑÎ°ù Ïú†ÎèÑÌïòÏó¨ Ìä∏Î†åÎìú Î†àÏù¥Î∏îÏùÑ Ï∂îÏ∂úÌï©ÎãàÎã§.",
    "prompt_text": "ÏïÑÎûò Ï†úÍ≥µÎêú ÌÖçÏä§Ìä∏Îì§ÏùÄ Í≥µÌÜµÏ†ÅÏúºÎ°ú ÏΩîÎìú Í∞í {code_value}Î•º Í∞ÄÏßÄÎäî Îâ¥Ïä§ Í∏∞ÏÇ¨/Ï†ïÎ≥¥Ïùò ÏùºÎ∂ÄÏûÖÎãàÎã§.\n\nÏù¥ Î™®Îì† ÌÖçÏä§Ìä∏Î•º Í∏∞Î∞òÏúºÎ°ú, Ìï¥Îãπ ÏΩîÎìúÏôÄ Í¥ÄÎ†®Îêú **Í∞ÄÏû• Ï§ëÏöîÌïú Í±∞ÏãúÏ†Å ÏãúÏû• ÎèôÌñ•**Í≥º **Ïû†Ïû¨Ï†Å ÏúÑÌóò/Í∏∞Ìöå ÏöîÏÜå**Î•º 100~200Ïûê ÏÇ¨Ïù¥Ïùò ÌïòÎÇòÏùò **ÌïúÍµ≠Ïñ¥ ÏöîÏïΩ(Silver Label)**ÏúºÎ°ú Ï†ïÎ¶¨Ìï¥ Ï£ºÏÑ∏Ïöî.\n\n**[ÌïµÏã¨ ÏöîÍµ¨ÏÇ¨Ìï≠]**\n1. **Í≤ΩÏ†ú ÌôòÍ≤Ω/Ï†ïÏ±Ö Î≥ÄÌôî**Í∞Ä Ìè¨Ìï®ÎêòÏñ¥Ïïº Ìï©ÎãàÎã§.\n2. **Í∞úÎ≥Ñ Í∏∞ÏóÖÎ™Ö Ïñ∏Í∏âÏùÄ ÏµúÏÜåÌôî**ÌïòÍ≥† ÏÇ∞ÏóÖ Ï†ÑÎ∞òÏùò Î∞©Ìñ•ÏÑ±ÏùÑ Ï†úÏãúÌï¥Ïïº Ìï©ÎãàÎã§.\n\nÏöîÏïΩ ÌòïÏãù: \"ÌïµÏã¨ Ï£ºÏ†ú: (ÏãúÏû• Î≥ÄÌôîÏôÄ Ï£ºÏöî Î¶¨Ïä§ÌÅ¨/Í∏∞ÌöåÏóê Ï¥àÏ†ê ÎßûÏ∂ò ÎÇ¥Ïö©)\"\n\n--- ÌÖçÏä§Ìä∏ Î™©Î°ù ---\n{combined_text}",
    "output_name": "silver_label_market_trend.csv"
  },
  {
    "prompt_number": 3,
    "prompt_context": "Í∞êÏÑ±/ÌÉúÎèÑ ÌèâÍ∞Ä: ÌÖçÏä§Ìä∏Ïùò Ï†ÑÎ∞òÏ†ÅÏù∏ Î∂ÑÏúÑÍ∏∞(Í∏çÏ†ï/Î∂ÄÏ†ï/Ï§ëÎ¶Ω)Î•º ÌååÏïÖÌïòÍ≥†, Í∑∏ Í∑ºÍ±∞Í∞Ä ÎêòÎäî ÌôúÎèô(Ìà¨Ïûê vs. ÌïòÎùΩ)ÏùÑ Î™ÖÏãúÌïòÎèÑÎ°ù ÏöîÍµ¨ÌïòÏó¨ Í∞êÏÑ± Îç∞Ïù¥ÌÑ∞ Î†àÏù¥Î∏îÏùÑ Ï∂îÏ∂úÌï©ÎãàÎã§.",
    "prompt_text": "ÏïÑÎûò Ï†úÍ≥µÎêú ÌÖçÏä§Ìä∏Îì§ÏùÑ Î∂ÑÏÑùÌïòÏó¨, Ìï¥Îãπ ÏΩîÎìú {code_value}ÏôÄ Í¥ÄÎ†®Îêú ÌôúÎèôÏùò **Ï†ÑÎ∞òÏ†ÅÏù∏ Í∞êÏÑ±(Í∏çÏ†ï/Î∂ÄÏ†ï/Ï§ëÎ¶Ω)**ÏùÑ ÌååÏïÖÌïòÍ≥† Í∑∏ Í∑ºÍ±∞Î•º ÏöîÏïΩÌïòÏÑ∏Ïöî.\n\nÏöîÏïΩÏùÄ Îã§Ïùå ÎÑ§ Í∞ÄÏßÄ ÏöîÏÜåÎ•º Î™®Îëê Ìè¨Ìï®ÌïòÎäî 100~200Ïûê ÏÇ¨Ïù¥Ïùò ÌïòÎÇòÏùò **ÌïúÍµ≠Ïñ¥ ÏöîÏïΩ(Silver Label)**ÏúºÎ°ú Ï†ïÎ¶¨Ìï¥ Ï£ºÏÑ∏Ïöî.\n\n1. **Í∞ÄÏû• ÎπàÎ≤àÌïòÍ≤å Ïñ∏Í∏âÎêú Í∏∞ÏóÖ ÌôúÎèô** (Ïòà: ÏÑ±Ïû•, Ìà¨Ïûê, ÌïòÎùΩ, Íµ¨Ï°∞Ï°∞Ï†ï)\n2. **Ï†ÑÎ∞òÏ†ÅÏù∏ Í∞êÏÑ± ÌèâÍ∞Ä** (Í∏çÏ†ï, Î∂ÄÏ†ï, ÎòêÎäî ÌòºÏû¨)\n3. **Í∞êÏÑ± ÌèâÍ∞ÄÏùò Í∑ºÍ±∞**Í∞Ä ÎêòÎäî Ï£ºÏöî ÏÇ¨Í±¥ (Í∏çÏ†ïÏ†Å/Î∂ÄÏ†ïÏ†Å)\n\nÏöîÏïΩ ÌòïÏãù: \"ÌïµÏã¨ Ï£ºÏ†ú: (Ï†ÑÎ∞òÏ†ÅÏù∏ Í∞êÏÑ± ÌèâÍ∞ÄÏôÄ Í∑∏ Í∑ºÍ±∞Î•º Î™ÖÌôïÌûà Ï†úÏãúÌïú ÎÇ¥Ïö©)\"\n\n--- ÌÖçÏä§Ìä∏ Î™©Î°ù ---\n{combined_text}",
    "output_name": "silver_label_sentiment_focus.csv"
  }
]

for index, p in enumerate(prompts):
    concated_prompt = p["prompt_context"] + p["prompt_text"]

# Î∞òÎ≥µÎ¨∏ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Ìïú Ï§ÑÏî© Ï≤òÎ¶¨
    for i, index in enumerate(group_indices):
        row = grouped.loc[index]
        text_list = row['text_list']
        group_key = row[GROUPING_KEY]
        
        print(f"\nüöÄ {i+1}/{total_groups} Ï≤òÎ¶¨ Ï§ë - Code: {group_key} (ÌÖçÏä§Ìä∏ {len(text_list)}Í∞ú)")

        # 1. generate_silver_label Ìï®Ïàò Ìò∏Ï∂ú
        silver_label = generate_silver_label(text_list, group_key, client, model, concated_prompt)
        
        # 2. Í≤∞Í≥º Ï†ÄÏû•
        grouped.loc[index, 'silver_label'] = silver_label
    
        if silver_label.startswith("ÏöîÏïΩ Ïò§Î•ò:"):
            print(f"‚ùå Ïò§Î•ò Î∞úÏÉù: {silver_label}")
        else:
            print("‚úÖ Silver Label ÏÉùÏÑ± Î∞è Ï†ÄÏû• ÏôÑÎ£å.")

        time.sleep(0.5)
        
    print("\n‚úÖ Silver Label ÏÉùÏÑ± ÏôÑÎ£å.")

    # --- 5. ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏóê Silver Label Î≥ëÌï© ---

    # Î≥ëÌï©ÏùÑ ÏúÑÌï¥ 'silver_label'Í≥º GROUPING_KEYÎßå Ìè¨Ìï®ÌïòÎäî Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ Ï§ÄÎπÑ
    silver_labels_df = grouped[[GROUPING_KEY, 'silver_label']]

    # ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏóê Î≥ëÌï©
    df_merged = pd.merge(df.drop(columns=['text']), silver_labels_df, on=GROUPING_KEY, how='left')

    # 'silver_label'ÏùÑ ÏÉà 'text'Î°ú ÏÇ¨Ïö©ÌïòÍ≥† Ïù¥Î¶ÑÏùÑ Î≥ÄÍ≤Ω
    df_merged.rename(columns={'silver_label': 'text'}, inplace=True)

    # Ïó¥ ÏàúÏÑú Ï°∞Ï†ï (ÏõêÎ≥∏Í≥º Ïú†ÏÇ¨ÌïòÍ≤å)
    cols = list(df_merged.columns)
    cols.remove('text')
    df_final = df_merged[cols + ['text']]

    # --- 6. Í≤∞Í≥º Ï†ÄÏû• ---

    output_file_path = p["output_name"]
    try:
        df_final.to_csv(output_file_path, index=False, encoding='utf-8')
        print(f"\nüéâ ÏµúÏ¢Ö Í≤∞Í≥ºÍ∞Ä '{output_file_path}'Ïóê Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§.")
        print("üëâ Ïù¥Ï†ú Ïù¥ ÌååÏùºÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÌõÑÏÜç Î∂ÑÏÑùÏùÑ ÏßÑÌñâÌï† Ïàò ÏûàÏäµÎãàÎã§.")
    except Exception as e:
        print(f"‚ùå Ïò§Î•ò: ÏµúÏ¢Ö CSV ÌååÏùº Ï†ÄÏû• Ï§ë Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {e}")





import os
import math
from typing import List

import numpy as np
import pandas as pd

print("pandas version:", pd.__version__)

# s3 Í∏∞Î≥∏ ÏÑ§Ï†ï
# BUCKET = "sagemaker-us-west-2-327784329358"
# S3_BASE = f"s3://{BUCKET}"

# ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞: S3ÏóêÏÑú Î∞îÎ°ú ÏùΩÍ∏∞
# PRICE_OHLCV_PATH = f"{S3_BASE}/prepared_data/price_ohlcv.csv"
# VQ_CODE_CSV_PATH = f"{S3_BASE}/vq_vae_outputs/vq_codes_spy.csv"
# NEWS_CSV_PATH = f"{S3_BASE}/prepared_data/sp500_headlines_2008_2024.csv"

# Ï∂úÎ†• JSONL: Î°úÏª¨Ïóê ÎßåÎì§Í≥† S3Î°ú ÏóÖÎ°úÎìú
# OUTPUT_JSONL_LOCAL_PATH = "/tmp/market_commentary_train.jsonl"
# OUTPUT_JSONL_S3_KEY = "prepared_data/market_commentary_train.jsonl"

# Data Path
OUTPUT_JSONL_LOCAL_PATH = "prepared_data/train.jsonl"
SILVER_LABEL_PATH = "silver_label_market_trend.csv"
PRICE_OHLCV_PATH = "spy_2023_2024.csv"


df_silver_label = pd.read_csv(SILVER_LABEL_PATH)
df_price = pd.read_csv(PRICE_OHLCV_PATH)


# required_cols = {"date", "close"}
# if not required_cols.issubset(df_price.columns):
    # raise ValueError("prices_ohlcv.csvÏóê date, close Ïª¨ÎüºÏù¥ ÌïÑÏöîÌï©ÎãàÎã§.")

df_silver_label['date'] = pd.to_datetime(df_silver_label['date']).dt.date

df_price['date'] = pd.to_datetime(df_price['date'])
df_price['day'] = df_price['date'].dt.date
df_price = df_price.groupby('day').tail(1)

def make_summary(row):
    return (
        f"open: {row['1. open']}, "
        f"high: {row['2. high']}, "
        f"low: {row['3. low']}, "
        f"close: {row['4. close']}, "
        f"volume: {row['5. volume']}"
    )

df_price["ohlcv_summary"] = df_price.apply(make_summary, axis=1)
df_price = df_price[['day', 'ohlcv_summary']].copy()
df_price = df_price.rename(columns={'day': 'date'})
df_price['date'] = pd.to_datetime(df_price['date']).dt.date

# df_price["date"] = pd.to_datetime(df_price["date"])
# df_price = df_price.sort_values("date").reset_index(drop=True)

# Ïó¨Í∏∞ÏÑú ÏàòÏùµÎ•†, Î≥ÄÎèôÏÑ± Îì± ÌååÏÉù ÌîºÏ≤ò ÏÉùÏÑ±
# df_price["ret_1d"] = df_price["close"].pct_change()
# df_price["log_ret"] = np.log(df_price["close"]).diff()
# df_price["close_ma_5"] = df_price["close"].rolling(window=5).mean()
# df_price["close_ma_20"] = df_price["close"].rolling(window=20).mean()
# df_price["vol_5"] = df_price["log_ret"].rolling(window=5).std()
# df_price["vol_20"] = df_price["log_ret"].rolling(window=20).std()

# NaN Ï†úÍ±∞
# df_price = df_price.dropna().reset_index(drop=True)
# print("[PRICE] head:\n", df_price.head())


# VQ-VAE ÏΩîÎìúÏôÄ price join
# df_vq = pd.read_csv(VQ_CODE_CSV_PATH)
# df_vq["date"] = pd.to_datetime(df_vq["date"])

# df_merged = pd.merge(df_vq, df_price, on="date", how="inner")
# df_merged = df_merged.sort_values("date").reset_index(drop=True)

# News Headline join
# df_news = pd.read_csv("NEWS_CSV_PATH")
# df_news["Date"] = pd.to_datetime(df_news["Date"])

""" news_grouped = (
    df_news.groupby("Date")["Title"]
    .apply(list)
    .reset_index()
    .rename(columns={"Date": "date", "Title": "titles"})
) """

# merge all
df_all = pd.merge(
    df_silver_label,
    df_price,                # VQ ÏΩîÎìú + Í∞ÄÍ≤© ÏöîÏïΩ
    on="date",
    how="left",
)


def code_to_str(row):
    z_values = [row[f"z_{i}"] for i in range(8)]
    z_str = ", ".join([f"{v:.6f}" for v in z_values])
    return f"{z_str}"

df_all["code_str"] = df_all.apply(code_to_str, axis=1)


df_all.head()


def build_prompt(code_value: int, code_vector: str, ohlcv_summary: str):
    prompt = (
        "### ÏΩîÎìúÎ∂Å\n" + str(code_value) +"\n\n"
        "### ÏΩîÎìúÎ∂Å Î≤°ÌÑ∞\n" + str(code_vector) + "\n\n"
        "### OHLCV ÏãúÍ≥ÑÏó¥\n" + ohlcv_summary + "\n\n"

        "ÏúÑÏùò Code Î≤°ÌÑ∞ Ï†ïÎ≥¥ÏôÄ OHLCV ÏãúÍ≥ÑÏó¥ÏùÑ Î∞îÌÉïÏúºÎ°ú, Ìï¥Îãπ Íµ¨Í∞ÑÏùò ÏãúÏû• ÏõÄÏßÅÏûÑÏùÑ ÏÑ§Î™ÖÌï† Ïàò ÏûàÎäî ÌïúÍµ≠Ïñ¥ Îâ¥Ïä§ ÏöîÏïΩÏùÑ 2~3 Î¨∏Ïû•ÏúºÎ°ú ÏûëÏÑ±ÌïòÎùº."
        "Îç∞Ïù¥ÌÑ∞Ïóê Í∏∞Î∞òÌïú Î≥¥ÏàòÏ†Å ÏÑ§Î™ÖÎßå ÏÇ¨Ïö©ÌïòÍ≥†, ÏûÖÎ†•Ïóê ÏóÜÎäî ÏÇ¨Í±¥ÏùÑ ÏÉàÎ°ú ÎßåÎì§Ïñ¥ÎÇ¥ÏßÄ ÎßàÎùº."
    )
    return prompt
    
# ÌîÑÎ°¨ÌîÑÌä∏ Ïª¨Îüº ÏÉùÏÑ± (Î†àÏù¥Î∏îÏùÄ placeholder)
df_all["prompt"] = df_all.apply(
    lambda row: build_prompt(
        code_value=row["code"],
        code_vector=row["code_str"],
        ohlcv_summary=row["ohlcv_summary"],
    ),
    axis=1,
)

# Ïã§Ï†ú ÌîÑÎ°úÏ†ùÌä∏ÏóêÏÑúÎäî ÏïÑÎûò completionÏùÑ ÏÇ¨Îûå/teacher LLMÏúºÎ°ú Ï±ÑÏõåÎÑ£Ïñ¥Ïïº Ìï®
# silver labelÏùÑ llm apiÎ°ú ÏÇ¨Ïö©ÌïòÍ±∞ÎÇò ÏÇ¨Îûå ÏÜêÏúºÎ°ú gole labelÏùÑ Î∂ôÏó¨ ÎÑ£Ïñ¥Ïïº Ìï®
df_all["completion"] = df_all["text"]

print("[ALL with prompt/completion] head:\n",
      df_all[["date", "code", "prompt", "completion"]].head())



# import boto3
import json

def export_jsonl(df: pd.DataFrame, out_path: str) -> None:
    os.makedirs(os.path.dirname(out_path) or ".", exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        for row in df.itertuples():
            rec = {
                "prompt": getattr(row, "prompt"),
                "completion": getattr(row, "completion"),
                "date": str(getattr(row, "date")),
            }
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")
    print(f"Saved JSONL to {out_path}")

export_jsonl(df_all, OUTPUT_JSONL_LOCAL_PATH)

# upload to s3 bucket
# s3 = boto3.client("s3")
# s3.upload_file(OUTPUT_JSONL_LOCAL_PATH, BUCKET, OUTPUT_JSONL_S3_KEY)

# print(f"Uploaded to s3://{BUCKET}/{OUTPUT_JSONL_S3_KEY}")





import os
import json
from dataclasses import dataclass
from typing import Dict, List

import random
import numpy as np
import torch
from torch.utils.data import Dataset

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    default_data_collator,
)

from peft import LoraConfig, get_peft_model



@dataclass
class TrainConfig:
    # 1) Î™®Îç∏ & Îç∞Ïù¥ÌÑ∞
    model_name_or_path: str = "meta-llama/Meta-Llama-3-8B-Instruct"  # ÏõêÌïòÎäî Î≤†Ïù¥Ïä§ LLM Ïù¥Î¶Ñ
    train_jsonl: str = "prepared_data/train.jsonl"          # ÌïôÏäµÏãúÌÇ¨ json prompt
    
    # Í≤∞Í≥º Ï†ÄÏû• ÏúÑÏπò: EBS Î≥ºÎ•® ÎÇ¥ ÌòÑÏû¨ ÏûëÏóÖ ÎîîÎ†âÌÜ†Î¶¨ Í∏∞Ï§Ä "./model"
    output_dir: str = "./model"

    # 2) ÌÜ†ÌÅ¨ÎÇòÏù¥Ï¶à & Í∏∏Ïù¥
    max_length: int = 200

    # 3) ÌïôÏäµ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞
    per_device_train_batch_size: int = 1
    gradient_accumulation_steps: int = 16
    num_train_epochs: int = 3
    learning_rate: float = 2e-4
    warmup_ratio: float = 0.03

    # 4) Î°úÍπÖ & Ï†ÄÏû•
    logging_steps: int = 50
    save_steps: int = 500
    seed: int = 42
    # 5) Í∏∞ÌÉÄ
    dataloader_num_workers: int = 2  # ColabÏù¥Î©¥ 0~2 Ï†ïÎèÑÎ°ú Ïú†ÏßÄ

    # device_map="auto"
    # load_in_4bit=True

cfg = TrainConfig()
cfg



def resolve_jsonl_path(path: str) -> str:
    """
    - s3://bucket/key ÌòïÏãùÏù¥Î©¥ /tmp/train.jsonl Î°ú Îã§Ïö¥Î°úÎìú ÌõÑ Ìï¥Îãπ Í≤ΩÎ°ú Î∞òÌôò
    - Í∑∏ Ïô∏ÏóêÎäî Î°úÏª¨ Í≤ΩÎ°ú Í∑∏ÎåÄÎ°ú Î∞òÌôò
    """
    if path.startswith("s3://"):
        no_scheme = path[5:]
        bucket, key = no_scheme.split("/", 1)

        local_path = "/tmp/train.jsonl"
        os.makedirs(os.path.dirname(local_path), exist_ok=True)

        s3 = boto3.client("s3")
        s3.download_file(bucket, key, local_path)
        print(f"[INFO] downloaded {path} -> {local_path}")
        return local_path
    else:
        return path


def seed_everything(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


seed_everything(cfg.seed)



class JsonlSftDataset(Dataset):
    """
    SFTÏö© JSONL Dataset.
    Í∞Å rowÎäî {"prompt": "...", "completion": "..."} ÌòïÌÉúÏó¨Ïïº Ìï®.
    Î∞òÌôòÎêòÎäî input_ids, attention_mask, labelsÎäî Î™®Îëê ÎèôÏùº Í∏∏Ïù¥Î•º Î≥¥Ïû•Ìï¥Ïïº ÌïúÎã§.
    """

    def __init__(self, jsonl_path, tokenizer, max_length=1024):
        self.samples = []
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                if not line.strip():
                    continue
                obj = json.loads(line)
                self.samples.append(obj)

        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        item = self.samples[idx]
        prompt = " ".join(str(item["prompt"]).split())
        completion = " ".join(str(item["completion"]).split())

        # 1) ÌîÑÎ°¨ÌîÑÌä∏ + completion Ìï©ÏπòÍ∏∞
        full_text = prompt + completion

        # 2) tokenizerÎ°ú ÏùºÍ¥Ñ encoding (truncation=True)
        enc = self.tokenizer(
            full_text,
            truncation=True,
            max_length=self.max_length,
            add_special_tokens=True,
        )

        input_ids = enc["input_ids"]
        attn_mask = enc["attention_mask"]

        # 3) promptÎßå tokenizeÌï¥ÏÑú Í∏∏Ïù¥ ÌååÏïÖ
        prompt_ids = self.tokenizer(
            prompt,
            add_special_tokens=False,
        )["input_ids"]

        # BOS ÌÜ†ÌÅ∞ Î≥¥Ï†ï
        bos_extra = 1 if (len(input_ids) > 0 and input_ids[0] == self.tokenizer.bos_token_id) else 0

        prompt_len = len(prompt_ids) + bos_extra

        # 4) labels = input_ids Î≥µÏÇ¨
        labels = input_ids.copy()

        # prompt Íµ¨Í∞Ñ -100
        for i in range(prompt_len):
            if i < len(labels):
                labels[i] = -100

        # ‚Äª Ïó¨Í∏∞ÏÑú Ï§ëÏöîÌïú Î∂ÄÎ∂Ñ: 
        # input_ids, attention_mask, labels Î™®Îëê Ï†ïÌôïÌûà ÎèôÏùº Í∏∏Ïù¥
        # collatorÍ∞Ä ÎÇòÏ§ëÏóê batch padding Ìï† Í≤ÉÏù¥ÎØÄÎ°ú DatasetÏóêÏÑúÎäî Í∏∏Ïù¥Î•º ÎßûÏ∂îÍ∏∞Îßå ÌïòÎ©¥ ÎêúÎã§.

        assert len(input_ids) == len(labels), f"label mismatch: {len(input_ids)} vs {len(labels)}"

        return {
            "input_ids": torch.tensor(input_ids, dtype=torch.long),
            "attention_mask": torch.tensor(attn_mask, dtype=torch.long),
            "labels": torch.tensor(labels, dtype=torch.long),
        }



from huggingface_hub import login
from huggingface_hub import HfApi
from huggingface_hub import hf_hub_download

api = HfApi()

try:
    hf_hub_download(
        repo_id="meta-llama/Meta-Llama-3-8B-Instruct",
        filename="config.json"
    )
    print("Access OK!")
except Exception as e:
    print("Access error:", e)

import requests

HF_TOKEN = "hf_xxxxxxxxx"   # ÎÑàÏùò Í≤É ÎÑ£Í∏∞
headers = {"Authorization": f"Bearer {HF_TOKEN}"}

url = "https://huggingface.co/api/models/meta-llama/Meta-Llama-3-8B-Instruct"
res = requests.get(url, headers=headers)

print(res.status_code)
# print(res.json())



# 1) ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä
tokenizer = AutoTokenizer.from_pretrained(
    cfg.model_name_or_path,
    use_fast=True,
)

# pad ÌÜ†ÌÅ∞ ÏÑ∏ÌåÖ (ÏóÜÎäî Î™®Îç∏Îì§ ÎåÄÎπÑ)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"


# 2) Î™®Îç∏ Î°úÎìú
model = AutoModelForCausalLM.from_pretrained(
    cfg.model_name_or_path,
    torch_dtype=torch.bfloat16,
    device_map="cuda",  # GPU ÏûàÏúºÎ©¥ GPU, ÏóÜÏúºÎ©¥ CPU
)

model.config.pad_token_id = tokenizer.pad_token_id
model.config.use_cache = False  # ÌïôÏäµ ÏãúÏóêÎäî False Í∂åÏû•

# 3) LoRA ÏÑ§Ï†ï
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    # LLaMA Í≥ÑÏó¥ Í∏∞Ï§Ä. Îã§Î•∏ Î™®Îç∏Ïù¥Î©¥ target_modules Ïù¥Î¶ÑÎßå Î∞îÍøîÏ£ºÎ©¥ Îê®.
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


import torch
from torch.nn.utils.rnn import pad_sequence

# pad tokenÏù¥ ÏóÜÎäî Î™®Îç∏Ïù¥Î©¥ eosÎ•º padÎ°ú Ïû¨ÏÇ¨Ïö©
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id

def sft_collate_fn(batch):
    """
    batch: list of {"input_ids": 1D tensor, "attention_mask": 1D tensor, "labels": 1D tensor}
    Î•º Î∞õÏïÑÏÑú, paddingÎêú Î∞∞ÏπòÎ•º Î∞òÌôò.
    """

    input_ids_list = [item["input_ids"] for item in batch]
    attention_mask_list = [item["attention_mask"] for item in batch]
    labels_list = [item["labels"] for item in batch]

    # Î∞∞Ïπò ÏïàÏóêÏÑú Í∞ÄÏû• Í∏¥ Í∏∏Ïù¥Ïóê ÎßûÏ∂∞ Ìå®Îî©
    input_ids_padded = pad_sequence(
        input_ids_list,
        batch_first=True,
        padding_value=tokenizer.pad_token_id,
    )

    attention_mask_padded = pad_sequence(
        attention_mask_list,
        batch_first=True,
        padding_value=0,          # Ìå®Îî© ÏúÑÏπòÎäî 0
    )

    labels_padded = pad_sequence(
        labels_list,
        batch_first=True,
        padding_value=-100,       # lossÏóêÏÑú Î¨¥ÏãúÎê† Í∞í
    )

    return {
        "input_ids": input_ids_padded,
        "attention_mask": attention_mask_padded,
        "labels": labels_padded,
    }



# S3 ÎòêÎäî Î°úÏª¨ Í≤ΩÎ°úÎ•º Ïã§Ï†ú Î°úÏª¨ ÌååÏùºÎ°ú resolve
train_jsonl_local = resolve_jsonl_path(cfg.train_jsonl)
print("Train JSONL local path:", train_jsonl_local)

# Dataset ÏÉùÏÑ±
from torch.utils.data import Subset
from transformers import DataCollatorWithPadding

# 1) Ï†ÑÏ≤¥ dataset ÏÉùÏÑ±
full_dataset = JsonlSftDataset(
    jsonl_path=train_jsonl_local,
    tokenizer=tokenizer,
    max_length=cfg.max_length,
)

# 2) train / val split (Ïòà: 90% / 10%)
n = len(full_dataset)
val_ratio = 0.1
val_size = max(1, int(n * val_ratio))

indices = list(range(n))
import random
random.shuffle(indices)

val_indices = indices[:val_size]
train_indices = indices[val_size:]

train_dataset = Subset(full_dataset, train_indices)
eval_dataset = Subset(full_dataset, val_indices)

print(f"Total samples: {n}, train: {len(train_dataset)}, val: {len(eval_dataset)}")

# 3) collator (Ïö∞Î¶¨Í∞Ä ÎßåÎì† labelsÎ•º Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ)
data_collator = DataCollatorWithPadding(
    tokenizer=tokenizer,
    padding=True,           # Î∞∞Ïπò ÎÇ¥ ÏµúÏû• Í∏∏Ïù¥Ïóê ÎßûÏ∂∞ Ìå®Îî©
    # max_length=cfg.max_length,  # (ÏÑ†ÌÉù) Í∞ïÏ†úÎ°ú Ïù¥ Í∏∏Ïù¥ÍπåÏßÄ Ìå®Îî©/Ïª∑ÌåÖÌïòÍ≥† Ïã∂ÏúºÎ©¥ ÏÑ§Ï†ï
)



import transformers
print(transformers.__version__)



# trainer
from transformers import TrainingArguments

os.makedirs(cfg.output_dir, exist_ok=True)

training_args = TrainingArguments(
    output_dir=cfg.output_dir,
    overwrite_output_dir=True,

    num_train_epochs=cfg.num_train_epochs,
    per_device_train_batch_size=cfg.per_device_train_batch_size,
    gradient_accumulation_steps=cfg.gradient_accumulation_steps,
    learning_rate=cfg.learning_rate,
    warmup_ratio=cfg.warmup_ratio,

    logging_steps=cfg.logging_steps,
    save_steps=cfg.save_steps,
    save_total_limit=3,

    fp16=False,
    bf16=True,

    seed=cfg.seed,
    report_to="none",
    dataloader_num_workers=cfg.dataloader_num_workers,

    # Ï∂îÍ∞Ä: epochÎßàÎã§ eval_loss Í≥ÑÏÇ∞
    eval_strategy="epoch",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,   # ‚Üê Ï∂îÍ∞Ä
    data_collator=data_collator,
)

print("Trainer ready.")




for i in range(0, len(train_dataset)):
    
    sample = train_dataset[i]
    print(len(sample["input_ids"]), len(sample["labels"]))


trainer.train()


import os
from datetime import datetime

def save_metrics_with_timestamp(df, prefix, out_dir="./logs"):
    # 1) ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±
    os.makedirs(out_dir, exist_ok=True)

    # 2) ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ ÏÉùÏÑ±
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")

    # 3) ÌååÏùº Í≤ΩÎ°ú ÏÉùÏÑ±
    filename = f"{prefix}_{ts}.csv"
    path = os.path.join(out_dir, filename)

    # 4) CSV Ï†ÄÏû•
    df.to_csv(path, index=False, encoding="utf-8-sig")

    print(f"Saved metrics to: {path}")
    return path


#epcohÎ≥Ñ eval_lossÏôÄ perplexity Ï†ïÎ¶¨
import math
import pandas as pd

log_history = trainer.state.log_history

rows = []
for log in log_history:
    # eval Îã®Í≥Ñ Î°úÍ∑∏Îßå Í≥®ÎùºÏÑú ÏÇ¨Ïö©
    if "eval_loss" in log:
        epoch = log.get("epoch", None)
        eval_loss = log["eval_loss"]
        perplexity = math.exp(eval_loss)
        rows.append({
            "epoch": epoch,
            "eval_loss": eval_loss,
            "perplexity": perplexity,
        })

df_metrics = pd.DataFrame(rows)
filename = "lora_metrics"
save_metrics_with_timestamp(df_metrics, filename)
print(df_metrics)



trainer.save_model(cfg.output_dir)      # LoRA Ïñ¥ÎåëÌÑ∞ Ìè¨Ìï®Ìïú Î™®Îç∏ Ï†ÄÏû•
tokenizer.save_pretrained(cfg.output_dir)

print("LoRA fine-tuning ÏôÑÎ£å. Ï†ÄÏû• ÏúÑÏπò:", cfg.output_dir)



#simple test
from peft import PeftModel

# Î≤†Ïù¥Ïä§ Î™®Îç∏ Îã§Ïãú Î°úÎìú (ÏÉà ÏÑ∏ÏÖòÏù¥Í±∞ÎÇò, Í≤ÄÏ¶ùÏö©)
base_model = AutoModelForCausalLM.from_pretrained(
    cfg.model_name_or_path,
    torch_dtype=torch.float16,
    device_map="cuda",
)
base_model.config.pad_token_id = tokenizer.pad_token_id
base_model.config.use_cache = True  # inferenceÏóêÏÑúÎäî TrueÎ°ú ÏºúÎèÑ Îê©ÎãàÎã§.

lora_model = PeftModel.from_pretrained(
    base_model,
    cfg.output_dir,
)
lora_model.eval()

def generate_comment(prompt_text: str, max_new_tokens: int = 256):
    inputs = tokenizer(
        prompt_text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=cfg.max_length,
    ).to(lora_model.device)

    with torch.no_grad():
        outputs = lora_model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            top_p=0.9,
            temperature=0.7,
            pad_token_id=tokenizer.pad_token_id,
        )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)


test_prompt = """ÎãπÏã†ÏùÄ ÎØ∏Íµ≠ Ï£ºÏãù Ïï†ÎÑêÎ¶¨Ïä§Ìä∏Îã§.

[Ï¢ÖÎ™© ÏΩîÎìú] NVDA
[Í∏∞Í∞Ñ] 2025-12-05

[OHLCV ÏöîÏïΩ]
- ÏãúÍ∞Ä: 370.5398
- Í≥†Í∞Ä: 372.5257
- Ï†ÄÍ∞Ä: 367.9274
- Ï¢ÖÍ∞Ä: 367.9755
- Í±∞ÎûòÎüâ: 7391068.0

[VQ ÏΩîÎìú ÏãúÌÄÄÏä§]
[0.021593764424324,	0.1893500536680221,	0.0058695869520306,	0.0947726070880889,	-9.261945399163935e-28,	0.0349617674946785,	0.4011827707290649,	0.5699447393417358]

[Îâ¥Ïä§ Ìó§ÎìúÎùºÏù∏]
- "ÎÇòÏä§Îã• Ï¢ÖÌï©ÏßÄÏàòÎäî ÌôîÏöîÏùº 1.5% Ïù¥ÏÉÅ Ìè≠ÎùΩÌïú ÌõÑ ÏàòÏöîÏùº 1.2% ÌïòÎùΩÌñàÏäµÎãàÎã§. S&P 500Í≥º Îã§Ïö∞Ï°¥Ïä§ ÏÇ∞ÏóÖÌèâÍ∑†ÏßÄÏàòÎäî Î™®Îëê 0.8% ÌïòÎùΩÌñàÏäµÎãàÎã§."

ÏúÑ Ï†ïÎ≥¥Î•º Ï¢ÖÌï©ÌïòÏó¨ 300~400Ïûê Î∂ÑÎüâÏùò ÏãúÌô© ÏΩîÎ©òÌä∏Î•º ÏûëÏÑ±ÌïòÎùº.

### ÎãµÎ≥Ä:
"""
comment = generate_comment(test_prompt)
print(comment)












