get_ipython().getoutput("pip install --upgrade pip wheel setuptools")

# Go 없이 설치 가능한 예전 버전 사용
get_ipython().getoutput("pip install "wandb<0.23"")



get_ipython().getoutput("pip install numpy")
get_ipython().getoutput("pip install pandas")
get_ipython().getoutput("pip install torch")
get_ipython().getoutput("pip install scikit-learn")
get_ipython().getoutput("pip install tqdm")



import os
import wandb

import numpy as np
import pandas as pd

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset
from torch.utils.data import DataLoader, random_split

from sklearn.preprocessing import MinMaxScaler

from tqdm import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"

print(device)





# from google.colab import drive
# drive.mount('/content/drive')


csv_filename = 'spy_2023_2024.csv'
csv_filepath = '/content/drive/MyDrive/2025 ML Project/datasets/spy_data.csv'

save_dir = 'prepared_data/'


wandb.finish()





wandb.login()






get_ipython().getoutput("pip install yfinance")


import yfinance as yf

snp500_data = yf.download("^GSPC", start="2005-01-01", end="2025-01-01", interval="30m")


snp500_data.columns = snp500_data.columns.droplevel(level='Ticker')
snp500_data


# Save Data to csv
snp500_data.to_csv(csv_filename, index=True, encoding='utf-8-sig')





get_ipython().run_line_magic("pip", " install alpha_vantage")


#from google.colab import userdata
#alphavantage_api = userdata.get('AlphaVantage')


csv_filename = 'spy_2023_data.csv'


# Get SPY data
from alpha_vantage.timeseries import TimeSeries
ts = TimeSeries(key='KFRTXN68V3DJZB4M', output_format='pandas')

month_data = []

for month in range(1, 13):
  month_str = f'{month:02d}'
  year_month = f'2023-{month_str}'

  try:
        print(f'Get {year_month} data')
        data, meta_data = ts.get_intraday(
            symbol='SPY',
            interval='30min',
            month=year_month,
            extended_hours='false',
            outputsize='full'
        )
        month_data.append(data)

        # api limit
        time.sleep(15)

  except Exception as e:
      print(f'Error with {year_month} : {e}')

if month_data:
    combined_data = pd.concat(month_data)
    combined_data.sort_index(inplace=True)
    print(combined_data.head())
    print(combined_data.tail())
else:
    print("No data")


combined_data


# Save Data to csv
combined_data.to_csv(csv_filename, index=True, encoding='utf-8-sig')


zeros_per_column = (combined_data == 0).sum()
zeros_per_column





# csv to DF
data = pd.read_csv(csv_filename)
data["date"] = pd.to_datetime(data["date"])


data


## 하루를 기준으로 정규화
def normalize_per_day(group):
  ohlc = ['1. open', '2. high', '3. low', '4. close']

  min_val = group[ohlc].min().min()
  max_val = group[ohlc].max().max()

  if max_val - min_val > 0:
    group[ohlc] = (group[ohlc] - min_val) / (max_val - min_val)
  else:
    group[ohlc] = 0.5

  ## 일단 volume도 하루 단위로 정규화
  min_vol = group['5. volume'].min()
  max_vol = group['5. volume'].max()
  group['5. volume'] = (group['5. volume'] - min_vol) / (max_vol - min_vol)

  return group

normalized_data = data.groupby(data['date'].dt.date).apply(normalize_per_day)


normalized_data


normalized_data.head(14)





class SPYDataSet(Dataset):
  def __init__(self, data, features, chunk_size):
    self.chunk_size = chunk_size

    arr = data[features].to_numpy(dtype=np.float32)
    self.arr = arr
    self.N, self.C = arr.shape

    self.num_chunks = self.N // self.chunk_size

  def __len__(self):
    return self.num_chunks

  def __getitem__(self, idx: int):
    start = idx * self.chunk_size
    end = start + self.chunk_size

    x = self.arr[start:end]
    x = torch.from_numpy(x).float().T
    return {"x": x, "idx": idx}





feature_cols = ['1. open', '2. high', '3. low', '4. close', '5. volume']

ds = SPYDataSet(normalized_data, features=feature_cols, chunk_size=13)
print(len(ds))


ds[0] # C(5) * T(13)





class Encoder(nn.Module):
  def __init__(self, input_dim, hidden_dim, latent_dim):
    super().__init__()
    self.conv = nn.Sequential(
        nn.Conv1d(input_dim, hidden_dim, kernel_size=5, stride=2, padding=2),
        nn.ReLU(),
        nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, stride=1, padding=2),
        nn.ReLU(),
        nn.Conv1d(hidden_dim, latent_dim, kernel_size=3, stride=1, padding=1),
        nn.ReLU(),
        nn.AdaptiveAvgPool1d(1)
    )

  def forward(self, x):
    return self.conv(x)


class Decoder(nn.Module):
  def __init__(self, latent_dim, hidden_dim, output_dim):
    super().__init__()
    self.deconv = nn.Sequential(
          nn.ConvTranspose1d(latent_dim, hidden_dim, kernel_size=7, stride=1, padding=0),
          nn.ReLU(),
          nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1),
          nn.ReLU(),
          nn.ConvTranspose1d(hidden_dim, hidden_dim, kernel_size=3, stride=2, padding=1, output_padding=0),
          nn.ReLU(),
          nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1),
          nn.ReLU(),
          nn.Conv1d(hidden_dim, output_dim, kernel_size=3, stride=1, padding=1)
      )

  def forward(self, z_q):
      return self.deconv(z_q)


class VectorQuantizer(nn.Module):
  """ num_embeddings: K (codebook size)
      embedding_dim:  D (code dimension)
      commitment_cost: beta in the paper """

  def __init__(self, num_embeddings, embedding_dim, commitment_cost):
    super().__init__()
    self.num_embeddings = num_embeddings
    self.embedding = nn.Embedding(num_embeddings, embedding_dim)
    self.commitment_cost = commitment_cost
    self.embedding.weight.data.uniform_(-1/num_embeddings, 1/num_embeddings)

  def forward(self, z):
    B, D, T = z.shape
    z_perm = z.permute(0, 2, 1).contiguous()
    z_flattened = z_perm.view(-1, D)

    e = self.embedding.weight
    z_sq = (z_flattened ** 2).sum(dim=1, keepdim=True)
    e_sq = (e ** 2).sum(dim=1)
    ze = z_flattened @ e.t()
    distances = z_sq + e_sq.unsqueeze(0) - 2 * ze

    encoding_indices = torch.argmin(distances, dim=1)
    z_q = self.embedding(encoding_indices).view(B, T, D).permute(0, 2, 1).contiguous()

    codebook_loss =  F.mse_loss(z_q, z.detach())
    commitment_loss = self.commitment_cost * F.mse_loss(z_q.detach(), z)
    vq_loss = codebook_loss + commitment_loss

    z_q = z + (z_q - z).detach()

    indices_bt = encoding_indices.view(B, T)
    return z_q, vq_loss, indices_bt


class VQVAE(nn.Module):
  def __init__(self, input_dim, hidden_dim, latent_dim, num_embeddings, commitment_cost):
    super().__init__()
    self.encoder = Encoder(input_dim, hidden_dim, latent_dim)
    self.vq = VectorQuantizer(num_embeddings, latent_dim, commitment_cost)
    self.decoder = Decoder(latent_dim, hidden_dim, input_dim)

  def forward(self, x):
    z_e = self.encoder(x)
    z_q, vq_loss, indices = self.vq(z_e)
    x_recon = self.decoder(z_q)
    return x_recon, vq_loss, indices, z_q


def evaluate(model, dataloader, device):
  index_list = []
  sum_recon, sum_vq = 0.0, 0.0
  n = 0

  model.eval()
  with torch.no_grad():
    for batch in tqdm(dataloader, desc="Evaluating", leave=False):
      X = batch["x"].to(device)

      x_recon, vq_loss, indices, _ = model(X)

      recon_loss = F.mse_loss(x_recon, X)

      batch_size = X.size(0)
      sum_recon += recon_loss.item() * batch_size
      sum_vq += vq_loss.item() * batch_size
      n += batch_size

      index_list.append(indices)

    mean_recon = sum_recon / max(n, 1)
    mean_vq = sum_vq / max(n, 1)
    sum_loss = mean_recon + mean_vq

    return mean_recon, mean_vq, sum_loss, index_list


wandb.init(
    project="2025 ML Project",
    entity="youani-korea-university",
    name="1 vector_8",
    config= {
        "batch_size": 32,
        "learning_rate": 3e-4,
        "epochs": 150,
        "num_embeddings": 32,
        "commitment_cost":0.3
    }
)


N = len(ds)
train_len = int(N * 0.7)
val_len = int(N * 0.15)
test_len = N - train_len - val_len

ds_train, ds_val, ds_test = random_split(ds, [train_len, val_len, test_len])
train_loader = DataLoader(ds_train, batch_size=wandb.config.batch_size, shuffle=True, drop_last=True)
val_loader   = DataLoader(ds_val, batch_size=wandb.config.batch_size, shuffle=False)
test_loader  = DataLoader(ds_test, batch_size=wandb.config.batch_size, shuffle=False)


# parameter setting
input_dim = 5
hidden_dim = 64
latent_dim = 8
num_embeddings = wandb.config.num_embeddings
commitment_cost = wandb.config.commitment_cost


# model training
lr = wandb.config.learning_rate
epochs = wandb.config.epochs
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = VQVAE(input_dim, hidden_dim, latent_dim, num_embeddings, commitment_cost)
model.to(device)
opt = torch.optim.Adam(model.parameters(), lr=lr)
recon_hist, vq_hist = [], []

best_val_loss = float("inf")
best_model_path = 'prepared_data/best_model_epoch.pt'

for epoch in range(1, epochs + 1):
  model.train()
  sum_recon, sum_vq = 0.0, 0.0
  n = 0

  for batch in tqdm(train_loader, desc="Train", leave=False):
    X = batch['x'].to(device)
    opt.zero_grad()
    x_recon, vq_loss, indices, _ = model(X)

    if x_recon.size(-1) != X.size(-1):
      print("Error: reconstruction size not equal to original")
      x_recon = F.interpolate(x_recon, size=X.size(-1), mode='linear', align_corners=False)

    recon_loss = F.mse_loss(x_recon, X)
    loss = recon_loss + vq_loss

    loss.backward()
    opt.step()

    batch_size = X.size(0)
    sum_recon += recon_loss.item() * batch_size
    sum_vq += vq_loss.item() * batch_size
    n += batch_size

  epoch_recon = sum_recon / max(n, 1)
  epoch_vq = sum_vq / max(n, 1)

  recon_hist.append(epoch_recon)
  vq_hist.append(epoch_vq)

  tqdm.write(f"[{epoch:03d}/{epochs:03d} Training] recon={epoch_recon:.6f} vq={epoch_vq:.6f}")

  val_recon, val_vq, val_loss, index_list = evaluate(model, val_loader, device)
  index_total = torch.cat(index_list).view(-1)
  usage_rate = len(index_total.unique()) / model.vq.num_embeddings
  wandb.log({
      f'valid_epoch_recon': val_recon,
      f'valid_epoch_vq': val_vq,
      f'usage_rate': usage_rate
  })
  tqdm.write(f"[{epoch:03d}/{epochs:03d} Validation] recon={val_recon:.6f} vq={val_vq:.6f}")

  if val_loss < best_val_loss:
    best_val_loss = val_loss
    best_model_path = os.path.join(save_dir, f"best_model_epoch.pt")
    torch.save(model.state_dict(),best_model_path)

print(f"Training Finished. Best model: {best_model_path} (val_loss: {best_val_loss:.6f})")


model.load_state_dict(torch.load(best_model_path))

test_recon, test_vq, test_loss, index_list = evaluate(model, test_loader, device)
wandb.log({
    f'test_epoch_recon': test_recon,
    f'test_epoch_vq': test_vq
})
print(f"[Test] recon={test_recon:.6f} vq={test_vq:.6f}")


wandb.finish()





all_x = []
all_assign = []

model.eval()
with torch.no_grad():
  for batch in test_loader:
    X = batch["x"].to(device)
    _, _, indices, _ = model(X)

    all_x.append(X.cpu())
    all_assign.append(indices.cpu())

X = torch.cat(all_x, dim=0)
assign = torch.cat(all_assign, dim=0)
assign


mask = (assign == 16).squeeze(1)
sample = X[mask]
sample.shape


selected = mask.nonzero(as_tuple=True)[0]
origin_indices = [int(test_loader.dataset[i.item()]["idx"]) for i in selected]


print(origin_indices)


selected_data = []
for idx in origin_indices:
    selected_rows = normalized_data.loc[(normalized_data.index.get_level_values(1) >= idx * 13) & (normalized_data.index.get_level_values(1) < idx * 13 + 13)]
    selected_data.append(selected_rows)

if selected_data:
    result_df = pd.concat(selected_data)
    result_df = result_df.reset_index(drop=True).sort_values(by='date', ascending=True)
    display(result_df)
else:
    print("No data selected for the given indices.")


result_df.head(15)


data_object = data.iloc[8*13:9*13]
data_object


from plotly.subplots import make_subplots
import plotly.graph_objects as go

target1 = 482
target2 = 69

data_object = data.iloc[target1*13 : (target1 + 1)*13]
data_object2 = data.iloc[target2*13 : (target2 + 1)*13]

fig = make_subplots(
    rows=1, cols=2,
    shared_yaxes=False,
    horizontal_spacing=0.05
)

fig.add_trace(go.Candlestick(x=data_object['date'],open=data_object['1. open'],high=data_object['2. high'],low=data_object['3. low'],close=data_object['4. close']), row=1, col=1)
fig.add_trace(go.Candlestick(x=data_object2['date'],open=data_object2['1. open'],high=data_object2['2. high'],low=data_object2['3. low'],close=data_object2['4. close']), row=1, col=2)
fig.show()


## Embedding Extract





wandb.init()





from tqdm import tqdm

# 전체 데이터셋용 DataLoader (shuffle=False 필수)
full_loader = DataLoader(
    ds,
    batch_size=wandb.config.batch_size,
    shuffle=False
)



model.eval()

all_idx = []
all_codes = []
all_embs = []  # z_q 벡터 (옵션)

with torch.no_grad():
    for batch in tqdm(full_loader, desc="Extract VQ codes"):
        X = batch["x"].to(device)          # (B, C, T)
        idxs = batch["idx"]                # 전역 윈도우 index

        x_recon, vq_loss, indices_bt, z_q = model(X)
        # indices_bt: (B, T=1) → 각 윈도우당 하나의 코드
        codes = indices_bt.squeeze(1).cpu().numpy()   # (B,)
        z_vec = z_q.squeeze(-1).cpu().numpy()         # (B, latent_dim)

        all_idx.extend(idxs.cpu().numpy().tolist())
        all_codes.extend(codes.tolist())
        all_embs.extend(z_vec.tolist())



print(ds[1])


import os
# import boto3

chunk_size = ds.chunk_size
rows = []
for idx, code, emb in zip(all_idx, all_codes, all_embs):
    start = idx * chunk_size
    end = start + chunk_size
    ts = normalized_data.iloc[end - 1]["date"]
    ts = ts.date()

    row = {"date": ts, "code": int(code)}
    for j, v in enumerate(emb):
        row[f"z_{j}"] = float(v)
    rows.append(row)

df_vq = pd.DataFrame(rows).sort_values("date").reset_index(drop=True)

# 2) 로컬 임시 파일 경로
local_path = "prepared_data/embedding.csv"
df_vq.to_csv(local_path, index=False, encoding="utf-8-sig")

# # 3) S3 버킷/키 설정
# bucket_name = "sagemaker-us-west-2-327784329358"
# s3_prefix = "vq_vae_outputs/vq_codes.csv" 

# s3 = boto3.client("s3")
# s3.upload_file(local_path, bucket_name, s3_prefix)

# print(f"Uploaded to s3://{bucket_name}/{s3_prefix}")



import boto3
s3 = boto3.client("s3")
buckets = s3.list_buckets()["Buckets"]
print([b["Name"] for b in buckets])



import sagemaker
session = sagemaker.Session()
print(session.default_bucket())




