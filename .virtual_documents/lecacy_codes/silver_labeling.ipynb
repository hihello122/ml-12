get_ipython().getoutput("pip install google")

get_ipython().getoutput("pip install google-genai pandas --upgrade")

get_ipython().getoutput("pip install pandas")

import pandas as pd
from google import genai
from google.genai.errors import APIError
import time

# --- 1. ì„¤ì • ë° ì´ˆê¸°í™” ---

# âš ï¸ ì—¬ê¸°ì— ì‹¤ì œ Gemini API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.
# í™˜ê²½ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•˜ì§€ë§Œ, í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì§ì ‘ ì…ë ¥í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
API_KEY = "AIzaSyC4s-EgCqUKKn5eVwnpx-7DlvDVdAp5qkw"
try:
    # client ê°ì²´ëŠ” for ë£¨í”„ ë°–ì—ì„œ í•œ ë²ˆë§Œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤.
    client = genai.Client(api_key=API_KEY)
    model = 'gemini-2.5-flash'  # ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´ flash ëª¨ë¸ ì‚¬ìš©
except Exception as e:
    print(f"Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    exit()

# --- 2. ë°ì´í„° ë¡œë“œ ---

file_path = 'prepared_data/final.csv'
try:
    df = pd.read_csv(file_path)
    print(f"âœ… íŒŒì¼ '{file_path}' ë¡œë“œ ì™„ë£Œ. ì´ {len(df)}ê°œ í–‰.")
except FileNotFoundError:
    print(f"âŒ ì˜¤ë¥˜: íŒŒì¼ '{file_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    exit()
except Exception as e:
    print(f"âŒ ì˜¤ë¥˜: CSV íŒŒì¼ì„ ì½ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    exit()

# ê·¸ë£¹í™”í•  í‚¤ ì»¬ëŸ¼
GROUPING_KEY = 'code'

if GROUPING_KEY not in df.columns:
    print(f"âŒ ì˜¤ë¥˜: ë°ì´í„°í”„ë ˆì„ì— ê·¸ë£¹í™” í‚¤ì¸ '{GROUPING_KEY}' ì—´ì´ ì—†ìŠµë‹ˆë‹¤.")
    exit()

# --- 3. Silver Label ìƒì„±ì„ ìœ„í•œ í•¨ìˆ˜ ì •ì˜ ---

def generate_silver_label(text_list, code_value, client_obj, model_name, prompt_input):
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì•½í•©ë‹ˆë‹¤.
    (API í´ë¼ì´ì–¸íŠ¸ ê°ì²´ë¥¼ ì¸ìˆ˜ë¡œ ë°›ë„ë¡ ìˆ˜ì •)
    """
    if not text_list:
        return ""

    # í…ìŠ¤íŠ¸ë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
    combined_text = "\n---\n".join(text_list)
    combined_text = combined_text[:100000]
    # í”„ë¡¬í”„íŠ¸ ì •ì˜
    prompt = f"""
    ì•„ë˜ì— ì œê³µëœ í…ìŠ¤íŠ¸ë“¤ì€ ê³µí†µì ìœ¼ë¡œ ì½”ë“œ ê°’ {code_value}ë¥¼ ê°€ì§€ëŠ” ë‰´ìŠ¤ ê¸°ì‚¬/ì •ë³´ì˜ ì¼ë¶€ì…ë‹ˆë‹¤.
    
    ì´ ëª¨ë“  í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ë‚´ìš©ê³¼ ì£¼ì œë¥¼ íŒŒì•…í•˜ì—¬, 100~200ì ì‚¬ì´ì˜ í•˜ë‚˜ì˜ **í•œêµ­ì–´ ìš”ì•½(Silver Label)**ìœ¼ë¡œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.
    ìš”ì•½ì€ ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¦…ë‹ˆë‹¤: "í•µì‹¬ ì£¼ì œ: (í•µì‹¬ ë‚´ìš©)"
    
    --- í…ìŠ¤íŠ¸ ëª©ë¡ ---
    {combined_text}
    """

    try:
        # Gemini API í˜¸ì¶œ
        response = client_obj.models.generate_content(
            model=model_name,
            contents=prompt
        )
        return response.text.strip()
    
    # í† í° ë§Œë£Œ ë˜ëŠ” ê¸°íƒ€ API ì˜¤ë¥˜ ë°œìƒ ì‹œ
    except APIError as e:
        # ì˜¤ë¥˜ ìƒì„¸ ì •ë³´ë¥¼ ë°˜í™˜í•˜ì—¬ ë‚˜ì¤‘ì— ì›ì¸ì„ ë¶„ì„í•  ìˆ˜ ìˆê²Œ í•¨
        return f"ìš”ì•½ ì˜¤ë¥˜: API Error - {e}"
    except Exception as e:
        return f"ìš”ì•½ ì˜¤ë¥˜: Unexpected Error - {e}"

# --- 4. ê·¸ë£¹ë³„ ìš”ì•½ ë° Silver Label ìƒì„± (ì§€ì—°/ë°˜ë³µë¬¸ ì ìš©) ---

# GROUPING_KEYë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”
grouped = df.groupby(GROUPING_KEY)['text'].apply(list).reset_index(name='text_list')
print(f"âš™ï¸ {GROUPING_KEY} ê°’ {len(grouped)}ê°œë¡œ ê·¸ë£¹í™” ì™„ë£Œ.")

# 'silver_label' ì—´ì„ ë¯¸ë¦¬ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
grouped['silver_label'] = None 

print("âœ¨ ê·¸ë£¹ë³„ Silver Label ìƒì„± ì¤‘... (API í˜¸ì¶œ ì‚¬ì´ì— **1ë¶„ ì§€ì—°**ì´ ì ìš©ë©ë‹ˆë‹¤.)")

# ì¸ë±ìŠ¤ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ë¥¼ ì‰½ê²Œ í™•ì¸
group_indices = grouped.index.tolist()
total_groups = len(group_indices)


prompts = [
  {
    "prompt_number": 1,
    "prompt_context": "ìƒì„¸ ë¶„ì„ ë° í•­ëª© ê°•ì œ: ìš”ì•½ì— ë°˜ë“œì‹œ 'ì£¼ìš” ì‚°ì—… ë¶„ì•¼/ê¸°ìˆ  íŠ¸ë Œë“œ', 'ì£¼ìš” ì´ìŠˆ/í™œë™ ìœ í˜•', 'ì „ë°˜ì ì¸ ì‹œì¥ ë¶„ìœ„ê¸°/ì˜í–¥' ì„¸ ê°€ì§€ ìš”ì†Œë¥¼ í¬í•¨í•˜ë„ë¡ ê°•ì œí•˜ì—¬ êµ¬ì²´ì ì¸ ë¶„ì„ ë°ì´í„°ë¥¼ í™•ë³´í•©ë‹ˆë‹¤.",
    "prompt_text": "ì•„ë˜ ì œê³µëœ í…ìŠ¤íŠ¸ë“¤ì€ ê³µí†µì ìœ¼ë¡œ ì½”ë“œ ê°’ {code_value}ë¥¼ ê°€ì§€ëŠ” ë‰´ìŠ¤ ê¸°ì‚¬/ì •ë³´ì˜ ì¼ë¶€ì…ë‹ˆë‹¤.\n\në‹¹ì‹ ì€ ì „ë¬¸ ë‰´ìŠ¤ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì´ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬, ë‹¤ìŒ ì„¸ ê°€ì§€ í•µì‹¬ í•­ëª©ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ëŠ” 100~200ì ì‚¬ì´ì˜ í•˜ë‚˜ì˜ **í•œêµ­ì–´ ìš”ì•½(Silver Label)**ìœ¼ë¡œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.\n\n1. **ì–¸ê¸‰ëœ ì£¼ìš” ì‚°ì—… ë¶„ì•¼ ë˜ëŠ” ê¸°ìˆ  íŠ¸ë Œë“œ** (ì˜ˆ: IT, í—¬ìŠ¤ì¼€ì–´, ESG, AI ë“±)\n2. **ì£¼ìš” ì´ìŠˆ/í™œë™ì˜ ìœ í˜•** (ì˜ˆ: ì¸ìˆ˜í•©ë³‘, ì‹ ì œí’ˆ ì¶œì‹œ, ì‹¤ì  ë°œí‘œ, ì •ì±… ë³€í™” ë“±)\n3. **ì „ë°˜ì ì¸ ì‹œì¥ ë¶„ìœ„ê¸° ë˜ëŠ” ì˜í–¥** (ì˜ˆ: ì„±ì¥ ê¸°ëŒ€, ë¶ˆí™•ì‹¤ì„± ì¦ëŒ€, ê·œì œ ê°•í™” ë“±)\n\nìš”ì•½ì€ ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¥´ë©°, êµ¬ì²´ì ì´ì–´ì•¼ í•©ë‹ˆë‹¤:\n\"í•µì‹¬ ì£¼ì œ: (í•µì‹¬ ë‚´ìš©. ìœ„ 3ê°€ì§€ í•­ëª©ì´ ëª¨ë‘ ë…¹ì•„ ìˆì–´ì•¼ í•¨)\"\n\n--- í…ìŠ¤íŠ¸ ëª©ë¡ ---\n{combined_text}",
    "output_name": "silver_label_detailed_analysis.csv"
  },
  {
    "prompt_number": 2,
    "prompt_context": "ì‹œì¥ ë™í–¥ ì¤‘ì‹¬ ìš”ì•½: ê°œë³„ ê¸°ì—… í™œë™ë³´ë‹¤ëŠ” ê±°ì‹œì  ê´€ì ì˜ 'ì‹œì¥ ë³€í™”'ì™€ 'ìœ„í—˜ ìš”ì†Œ'ì— ì´ˆì ì„ ë§ì¶° ìš”ì•½í•˜ë„ë¡ ìœ ë„í•˜ì—¬ íŠ¸ë Œë“œ ë ˆì´ë¸”ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.",
    "prompt_text": "ì•„ë˜ ì œê³µëœ í…ìŠ¤íŠ¸ë“¤ì€ ê³µí†µì ìœ¼ë¡œ ì½”ë“œ ê°’ {code_value}ë¥¼ ê°€ì§€ëŠ” ë‰´ìŠ¤ ê¸°ì‚¬/ì •ë³´ì˜ ì¼ë¶€ì…ë‹ˆë‹¤.\n\nì´ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, í•´ë‹¹ ì½”ë“œì™€ ê´€ë ¨ëœ **ê°€ì¥ ì¤‘ìš”í•œ ê±°ì‹œì  ì‹œì¥ ë™í–¥**ê³¼ **ì ì¬ì  ìœ„í—˜/ê¸°íšŒ ìš”ì†Œ**ë¥¼ 100~200ì ì‚¬ì´ì˜ í•˜ë‚˜ì˜ **í•œêµ­ì–´ ìš”ì•½(Silver Label)**ìœ¼ë¡œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.\n\n**[í•µì‹¬ ìš”êµ¬ì‚¬í•­]**\n1. **ê²½ì œ í™˜ê²½/ì •ì±… ë³€í™”**ê°€ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n2. **ê°œë³„ ê¸°ì—…ëª… ì–¸ê¸‰ì€ ìµœì†Œí™”**í•˜ê³  ì‚°ì—… ì „ë°˜ì˜ ë°©í–¥ì„±ì„ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤.\n\nìš”ì•½ í˜•ì‹: \"í•µì‹¬ ì£¼ì œ: (ì‹œì¥ ë³€í™”ì™€ ì£¼ìš” ë¦¬ìŠ¤í¬/ê¸°íšŒì— ì´ˆì  ë§ì¶˜ ë‚´ìš©)\"\n\n--- í…ìŠ¤íŠ¸ ëª©ë¡ ---\n{combined_text}",
    "output_name": "silver_label_market_trend.csv"
  },
  {
    "prompt_number": 3,
    "prompt_context": "ê°ì„±/íƒœë„ í‰ê°€: í…ìŠ¤íŠ¸ì˜ ì „ë°˜ì ì¸ ë¶„ìœ„ê¸°(ê¸ì •/ë¶€ì •/ì¤‘ë¦½)ë¥¼ íŒŒì•…í•˜ê³ , ê·¸ ê·¼ê±°ê°€ ë˜ëŠ” í™œë™(íˆ¬ì vs. í•˜ë½)ì„ ëª…ì‹œí•˜ë„ë¡ ìš”êµ¬í•˜ì—¬ ê°ì„± ë°ì´í„° ë ˆì´ë¸”ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.",
    "prompt_text": "ì•„ë˜ ì œê³µëœ í…ìŠ¤íŠ¸ë“¤ì„ ë¶„ì„í•˜ì—¬, í•´ë‹¹ ì½”ë“œ {code_value}ì™€ ê´€ë ¨ëœ í™œë™ì˜ **ì „ë°˜ì ì¸ ê°ì„±(ê¸ì •/ë¶€ì •/ì¤‘ë¦½)**ì„ íŒŒì•…í•˜ê³  ê·¸ ê·¼ê±°ë¥¼ ìš”ì•½í•˜ì„¸ìš”.\n\nìš”ì•½ì€ ë‹¤ìŒ ë„¤ ê°€ì§€ ìš”ì†Œë¥¼ ëª¨ë‘ í¬í•¨í•˜ëŠ” 100~200ì ì‚¬ì´ì˜ í•˜ë‚˜ì˜ **í•œêµ­ì–´ ìš”ì•½(Silver Label)**ìœ¼ë¡œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.\n\n1. **ê°€ì¥ ë¹ˆë²ˆí•˜ê²Œ ì–¸ê¸‰ëœ ê¸°ì—… í™œë™** (ì˜ˆ: ì„±ì¥, íˆ¬ì, í•˜ë½, êµ¬ì¡°ì¡°ì •)\n2. **ì „ë°˜ì ì¸ ê°ì„± í‰ê°€** (ê¸ì •, ë¶€ì •, ë˜ëŠ” í˜¼ì¬)\n3. **ê°ì„± í‰ê°€ì˜ ê·¼ê±°**ê°€ ë˜ëŠ” ì£¼ìš” ì‚¬ê±´ (ê¸ì •ì /ë¶€ì •ì )\n\nìš”ì•½ í˜•ì‹: \"í•µì‹¬ ì£¼ì œ: (ì „ë°˜ì ì¸ ê°ì„± í‰ê°€ì™€ ê·¸ ê·¼ê±°ë¥¼ ëª…í™•íˆ ì œì‹œí•œ ë‚´ìš©)\"\n\n--- í…ìŠ¤íŠ¸ ëª©ë¡ ---\n{combined_text}",
    "output_name": "silver_label_sentiment_focus.csv"
  }
]

for index, p in enumerate(prompts):
    concated_prompt = p["prompt_context"] + p["prompt_text"]

# ë°˜ë³µë¬¸ì„ ì‚¬ìš©í•˜ì—¬ í•œ ì¤„ì”© ì²˜ë¦¬
    for i, index in enumerate(group_indices):
        row = grouped.loc[index]
        text_list = row['text_list']
        group_key = row[GROUPING_KEY]
        
        print(f"\nğŸš€ {i+1}/{total_groups} ì²˜ë¦¬ ì¤‘ - Code: {group_key} (í…ìŠ¤íŠ¸ {len(text_list)}ê°œ)")

        # 1. generate_silver_label í•¨ìˆ˜ í˜¸ì¶œ
        silver_label = generate_silver_label(text_list, group_key, client, model, concated_prompt)
        
        # 2. ê²°ê³¼ ì €ì¥
        grouped.loc[index, 'silver_label'] = silver_label
    
        if silver_label.startswith("ìš”ì•½ ì˜¤ë¥˜:"):
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {silver_label}")
        else:
            print("âœ… Silver Label ìƒì„± ë° ì €ì¥ ì™„ë£Œ.")

        time.sleep(0.5)
        
    print("\nâœ… Silver Label ìƒì„± ì™„ë£Œ.")

    # --- 5. ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì— Silver Label ë³‘í•© ---

    # ë³‘í•©ì„ ìœ„í•´ 'silver_label'ê³¼ GROUPING_KEYë§Œ í¬í•¨í•˜ëŠ” ë°ì´í„°í”„ë ˆì„ ì¤€ë¹„
    silver_labels_df = grouped[[GROUPING_KEY, 'silver_label']]

    # ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì— ë³‘í•©
    df_merged = pd.merge(df.drop(columns=['text']), silver_labels_df, on=GROUPING_KEY, how='left')

    # 'silver_label'ì„ ìƒˆ 'text'ë¡œ ì‚¬ìš©í•˜ê³  ì´ë¦„ì„ ë³€ê²½
    df_merged.rename(columns={'silver_label': 'text'}, inplace=True)

    # ì—´ ìˆœì„œ ì¡°ì • (ì›ë³¸ê³¼ ìœ ì‚¬í•˜ê²Œ)
    cols = list(df_merged.columns)
    cols.remove('text')
    df_final = df_merged[cols + ['text']]

    # --- 6. ê²°ê³¼ ì €ì¥ ---

    output_file_path = p["output_name"]
    try:
        df_final.to_csv(output_file_path, index=False, encoding='utf-8')
        print(f"\nğŸ‰ ìµœì¢… ê²°ê³¼ê°€ '{output_file_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ğŸ‘‰ ì´ì œ ì´ íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ í›„ì† ë¶„ì„ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: ìµœì¢… CSV íŒŒì¼ ì €ì¥ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


get_ipython().getoutput("pip install --upgrade pip wheel setuptools")

# Go ì—†ì´ ì„¤ì¹˜ ê°€ëŠ¥í•œ ì˜ˆì „ ë²„ì „ ì‚¬ìš©
get_ipython().getoutput("pip install "wandb<0.23"")



get_ipython().getoutput("pip install numpy")
get_ipython().getoutput("pip install pandas")
get_ipython().getoutput("pip install torch")
get_ipython().getoutput("pip install scikit-learn")
get_ipython().getoutput("pip install tqdm")



import os
import wandb

import numpy as np
import pandas as pd

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset
from torch.utils.data import DataLoader, random_split

from sklearn.preprocessing import MinMaxScaler

from tqdm import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"

print(device)





# from google.colab import drive
# drive.mount('/content/drive')


csv_filename = 'spy_2023_2024.csv'
csv_filepath = '/content/drive/MyDrive/2025 ML Project/datasets/spy_data.csv'

save_dir = 'prepared_data/'


wandb.finish()





wandb.login()






get_ipython().getoutput("pip install yfinance")


import yfinance as yf

snp500_data = yf.download("^GSPC", start="2005-01-01", end="2025-01-01", interval="30m")


snp500_data.columns = snp500_data.columns.droplevel(level='Ticker')
snp500_data


# Save Data to csv
snp500_data.to_csv(csv_filename, index=True, encoding='utf-8-sig')





get_ipython().run_line_magic("pip", " install alpha_vantage")


#from google.colab import userdata
#alphavantage_api = userdata.get('AlphaVantage')


csv_filename = 'spy_2023_data.csv'


# Get SPY data
from alpha_vantage.timeseries import TimeSeries
ts = TimeSeries(key='KFRTXN68V3DJZB4M', output_format='pandas')

month_data = []

for month in range(1, 13):
  month_str = f'{month:02d}'
  year_month = f'2023-{month_str}'

  try:
        print(f'Get {year_month} data')
        data, meta_data = ts.get_intraday(
            symbol='SPY',
            interval='30min',
            month=year_month,
            extended_hours='false',
            outputsize='full'
        )
        month_data.append(data)

        # api limit
        time.sleep(15)

  except Exception as e:
      print(f'Error with {year_month} : {e}')

if month_data:
    combined_data = pd.concat(month_data)
    combined_data.sort_index(inplace=True)
    print(combined_data.head())
    print(combined_data.tail())
else:
    print("No data")


combined_data


# Save Data to csv
combined_data.to_csv(csv_filename, index=True, encoding='utf-8-sig')


zeros_per_column = (combined_data == 0).sum()
zeros_per_column





# csv to DF
data = pd.read_csv(csv_filename)
data["date"] = pd.to_datetime(data["date"])


data


## í•˜ë£¨ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”
def normalize_per_day(group):
  ohlc = ['1. open', '2. high', '3. low', '4. close']

  min_val = group[ohlc].min().min()
  max_val = group[ohlc].max().max()

  if max_val - min_val > 0:
    group[ohlc] = (group[ohlc] - min_val) / (max_val - min_val)
  else:
    group[ohlc] = 0.5

  ## ì¼ë‹¨ volumeë„ í•˜ë£¨ ë‹¨ìœ„ë¡œ ì •ê·œí™”
  min_vol = group['5. volume'].min()
  max_vol = group['5. volume'].max()
  group['5. volume'] = (group['5. volume'] - min_vol) / (max_vol - min_vol)

  return group

normalized_data = data.groupby(data['date'].dt.date).apply(normalize_per_day)


normalized_data


normalized_data.head(14)





class SPYDataSet(Dataset):
  def __init__(self, data, features, chunk_size):
    self.chunk_size = chunk_size

    arr = data[features].to_numpy(dtype=np.float32)
    self.arr = arr
    self.N, self.C = arr.shape

    self.num_chunks = self.N // self.chunk_size

  def __len__(self):
    return self.num_chunks

  def __getitem__(self, idx: int):
    start = idx * self.chunk_size
    end = start + self.chunk_size

    x = self.arr[start:end]
    x = torch.from_numpy(x).float().T
    return {"x": x, "idx": idx}





feature_cols = ['1. open', '2. high', '3. low', '4. close', '5. volume']

ds = SPYDataSet(normalized_data, features=feature_cols, chunk_size=13)
print(len(ds))


ds[0] # C(5) * T(13)





class Encoder(nn.Module):
  def __init__(self, input_dim, hidden_dim, latent_dim):
    super().__init__()
    self.conv = nn.Sequential(
        nn.Conv1d(input_dim, hidden_dim, kernel_size=5, stride=2, padding=2),
        nn.ReLU(),
        nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, stride=1, padding=2),
        nn.ReLU(),
        nn.Conv1d(hidden_dim, latent_dim, kernel_size=3, stride=1, padding=1),
        nn.ReLU(),
        nn.AdaptiveAvgPool1d(1)
    )

  def forward(self, x):
    return self.conv(x)


class Decoder(nn.Module):
  def __init__(self, latent_dim, hidden_dim, output_dim):
    super().__init__()
    self.deconv = nn.Sequential(
          nn.ConvTranspose1d(latent_dim, hidden_dim, kernel_size=7, stride=1, padding=0),
          nn.ReLU(),
          nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1),
          nn.ReLU(),
          nn.ConvTranspose1d(hidden_dim, hidden_dim, kernel_size=3, stride=2, padding=1, output_padding=0),
          nn.ReLU(),
          nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1),
          nn.ReLU(),
          nn.Conv1d(hidden_dim, output_dim, kernel_size=3, stride=1, padding=1)
      )

  def forward(self, z_q):
      return self.deconv(z_q)


class VectorQuantizer(nn.Module):
  """ num_embeddings: K (codebook size)
      embedding_dim:  D (code dimension)
      commitment_cost: beta in the paper """

  def __init__(self, num_embeddings, embedding_dim, commitment_cost):
    super().__init__()
    self.num_embeddings = num_embeddings
    self.embedding = nn.Embedding(num_embeddings, embedding_dim)
    self.commitment_cost = commitment_cost
    self.embedding.weight.data.uniform_(-1/num_embeddings, 1/num_embeddings)

  def forward(self, z):
    B, D, T = z.shape
    z_perm = z.permute(0, 2, 1).contiguous()
    z_flattened = z_perm.view(-1, D)

    e = self.embedding.weight
    z_sq = (z_flattened ** 2).sum(dim=1, keepdim=True)
    e_sq = (e ** 2).sum(dim=1)
    ze = z_flattened @ e.t()
    distances = z_sq + e_sq.unsqueeze(0) - 2 * ze

    encoding_indices = torch.argmin(distances, dim=1)
    z_q = self.embedding(encoding_indices).view(B, T, D).permute(0, 2, 1).contiguous()

    codebook_loss =  F.mse_loss(z_q, z.detach())
    commitment_loss = self.commitment_cost * F.mse_loss(z_q.detach(), z)
    vq_loss = codebook_loss + 0.5 * commitment_loss

    z_q = z + (z_q - z).detach()

    indices_bt = encoding_indices.view(B, T)
    return z_q, vq_loss, indices_bt


class VQVAE(nn.Module):
  def __init__(self, input_dim, hidden_dim, latent_dim, num_embeddings, commitment_cost):
    super().__init__()
    self.encoder = Encoder(input_dim, hidden_dim, latent_dim)
    self.vq = VectorQuantizer(num_embeddings, latent_dim, commitment_cost)
    self.decoder = Decoder(latent_dim, hidden_dim, input_dim)

  def forward(self, x):
    z_e = self.encoder(x)
    z_q, vq_loss, indices = self.vq(z_e)
    x_recon = self.decoder(z_q)
    return x_recon, vq_loss, indices, z_q


def evaluate(model, dataloader, device):
  index_list = []
  sum_recon, sum_vq = 0.0, 0.0
  n = 0

  model.eval()
  with torch.no_grad():
    for batch in tqdm(dataloader, desc="Evaluating", leave=False):
      X = batch["x"].to(device)

      x_recon, vq_loss, indices, _ = model(X)

      recon_loss = F.mse_loss(x_recon, X)

      batch_size = X.size(0)
      sum_recon += recon_loss.item() * batch_size
      sum_vq += vq_loss.item() * batch_size
      n += batch_size

      index_list.append(indices)

    mean_recon = sum_recon / max(n, 1)
    mean_vq = sum_vq / max(n, 1)
    sum_loss = mean_recon + mean_vq

    return mean_recon, mean_vq, sum_loss, index_list


wandb.init(
    project="2025 ML Project",
    entity="youani-korea-university",
    name="1 vector_8",
    config= {
        "batch_size": 32,
        "learning_rate": 3e-4,
        "epochs": 150,
        "num_embeddings": 32,
        "commitment_cost":0.3
    }
)


N = len(ds)
train_len = int(N * 0.7)
val_len = int(N * 0.15)
test_len = N - train_len - val_len

ds_train, ds_val, ds_test = random_split(ds, [train_len, val_len, test_len])
train_loader = DataLoader(ds_train, batch_size=wandb.config.batch_size, shuffle=True, drop_last=True)
val_loader   = DataLoader(ds_val, batch_size=wandb.config.batch_size, shuffle=False)
test_loader  = DataLoader(ds_test, batch_size=wandb.config.batch_size, shuffle=False)


# parameter setting
input_dim = 5
hidden_dim = 64
latent_dim = 8
num_embeddings = wandb.config.num_embeddings
commitment_cost = wandb.config.commitment_cost


# model training
lr = wandb.config.learning_rate
epochs = wandb.config.epochs
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = VQVAE(input_dim, hidden_dim, latent_dim, num_embeddings, commitment_cost)
model.to(device)
opt = torch.optim.Adam(model.parameters(), lr=lr)
recon_hist, vq_hist = [], []

best_val_loss = float("inf")
best_model_path = 'prepared_data/best_model_epoch.pt'

for epoch in range(1, epochs + 1):
  model.train()
  sum_recon, sum_vq = 0.0, 0.0
  n = 0

  for batch in tqdm(train_loader, desc="Train", leave=False):
    X = batch['x'].to(device)
    opt.zero_grad()
    x_recon, vq_loss, indices, _ = model(X)

    if x_recon.size(-1) != X.size(-1):
      print("Error: reconstruction size not equal to original")
      x_recon = F.interpolate(x_recon, size=X.size(-1), mode='linear', align_corners=False)

    recon_loss = F.mse_loss(x_recon, X)
    loss = recon_loss + vq_loss

    loss.backward()
    opt.step()

    batch_size = X.size(0)
    sum_recon += recon_loss.item() * batch_size
    sum_vq += vq_loss.item() * batch_size
    n += batch_size

  epoch_recon = sum_recon / max(n, 1)
  epoch_vq = sum_vq / max(n, 1)

  recon_hist.append(epoch_recon)
  vq_hist.append(epoch_vq)

  tqdm.write(f"[{epoch:03d}/{epochs:03d} Training] recon={epoch_recon:.6f} vq={epoch_vq:.6f}")

  val_recon, val_vq, val_loss, index_list = evaluate(model, val_loader, device)
  index_total = torch.cat(index_list).view(-1)
  usage_rate = len(index_total.unique()) / model.vq.num_embeddings
  wandb.log({
      f'valid_epoch_recon': val_recon,
      f'valid_epoch_vq': val_vq,
      f'usage_rate': usage_rate
  })
  tqdm.write(f"[{epoch:03d}/{epochs:03d} Validation] recon={val_recon:.6f} vq={val_vq:.6f}")

  if val_loss < best_val_loss:
    best_val_loss = val_loss
    best_model_path = os.path.join(save_dir, f"best_model_epoch.pt")
    torch.save(model.state_dict(),best_model_path)

print(f"Training Finished. Best model: {best_model_path} (val_loss: {best_val_loss:.6f})")


model.load_state_dict(torch.load(best_model_path))

test_recon, test_vq, test_loss, index_list = evaluate(model, test_loader, device)
wandb.log({
    f'test_epoch_recon': test_recon,
    f'test_epoch_vq': test_vq
})
print(f"[Test] recon={test_recon:.6f} vq={test_vq:.6f}")


wandb.finish()





all_x = []
all_assign = []

model.eval()
with torch.no_grad():
  for batch in test_loader:
    X = batch["x"].to(device)
    _, _, indices, _ = model(X)

    all_x.append(X.cpu())
    all_assign.append(indices.cpu())

X = torch.cat(all_x, dim=0)
assign = torch.cat(all_assign, dim=0)
assign


mask = (assign == 16).squeeze(1)
sample = X[mask]
sample.shape


selected = mask.nonzero(as_tuple=True)[0]
origin_indices = [int(test_loader.dataset[i.item()]["idx"]) for i in selected]


print(origin_indices)


selected_data = []
for idx in origin_indices:
    selected_rows = normalized_data.loc[(normalized_data.index.get_level_values(1) >= idx * 13) & (normalized_data.index.get_level_values(1) < idx * 13 + 13)]
    selected_data.append(selected_rows)

if selected_data:
    result_df = pd.concat(selected_data)
    result_df = result_df.reset_index(drop=True).sort_values(by='date', ascending=True)
    display(result_df)
else:
    print("No data selected for the given indices.")


result_df.head(15)


data_object = data.iloc[8*13:9*13]
data_object


from plotly.subplots import make_subplots
import plotly.graph_objects as go

target1 = 482
target2 = 69

data_object = data.iloc[target1*13 : (target1 + 1)*13]
data_object2 = data.iloc[target2*13 : (target2 + 1)*13]

fig = make_subplots(
    rows=1, cols=2,
    shared_yaxes=False,
    horizontal_spacing=0.05
)

fig.add_trace(go.Candlestick(x=data_object['date'],open=data_object['1. open'],high=data_object['2. high'],low=data_object['3. low'],close=data_object['4. close']), row=1, col=1)
fig.add_trace(go.Candlestick(x=data_object2['date'],open=data_object2['1. open'],high=data_object2['2. high'],low=data_object2['3. low'],close=data_object2['4. close']), row=1, col=2)
fig.show()


## Embedding Extract





wandb.init()





from tqdm import tqdm

# ì „ì²´ ë°ì´í„°ì…‹ìš© DataLoader (shuffle=False í•„ìˆ˜)
full_loader = DataLoader(
    ds,
    batch_size=wandb.config.batch_size,
    shuffle=False
)



model.eval()

all_idx = []
all_codes = []
all_embs = []  # z_q ë²¡í„° (ì˜µì…˜)

with torch.no_grad():
    for batch in tqdm(full_loader, desc="Extract VQ codes"):
        X = batch["x"].to(device)          # (B, C, T)
        idxs = batch["idx"]                # ì „ì—­ ìœˆë„ìš° index

        x_recon, vq_loss, indices_bt, z_q = model(X)
        # indices_bt: (B, T=1) â†’ ê° ìœˆë„ìš°ë‹¹ í•˜ë‚˜ì˜ ì½”ë“œ
        codes = indices_bt.squeeze(1).cpu().numpy()   # (B,)
        z_vec = z_q.squeeze(-1).cpu().numpy()         # (B, latent_dim)

        all_idx.extend(idxs.cpu().numpy().tolist())
        all_codes.extend(codes.tolist())
        all_embs.extend(z_vec.tolist())



print(ds[1])


import os
# import boto3

chunk_size = ds.chunk_size
rows = []
for idx, code, emb in zip(all_idx, all_codes, all_embs):
    start = idx * chunk_size
    end = start + chunk_size
    ts = normalized_data.iloc[end - 1]["date"]
    ts = ts.date()

    row = {"date": ts, "code": int(code)}
    for j, v in enumerate(emb):
        row[f"z_{j}"] = float(v)
    rows.append(row)

df_vq = pd.DataFrame(rows).sort_values("date").reset_index(drop=True)

# 2) ë¡œì»¬ ì„ì‹œ íŒŒì¼ ê²½ë¡œ
local_path = "prepared_data/embedding.csv"
df_vq.to_csv(local_path, index=False, encoding="utf-8-sig")

# # 3) S3 ë²„í‚·/í‚¤ ì„¤ì •
# bucket_name = "sagemaker-us-west-2-327784329358"
# s3_prefix = "vq_vae_outputs/vq_codes.csv" 

# s3 = boto3.client("s3")
# s3.upload_file(local_path, bucket_name, s3_prefix)

# print(f"Uploaded to s3://{bucket_name}/{s3_prefix}")



import boto3
s3 = boto3.client("s3")
buckets = s3.list_buckets()["Buckets"]
print([b["Name"] for b in buckets])



import sagemaker
session = sagemaker.Session()
print(session.default_bucket())




